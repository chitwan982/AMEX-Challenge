{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e23969e7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-26T15:21:21.125132Z",
     "iopub.status.busy": "2025-07-26T15:21:21.124470Z",
     "iopub.status.idle": "2025-07-26T15:21:29.875631Z",
     "shell.execute_reply": "2025-07-26T15:21:29.874754Z"
    },
    "papermill": {
     "duration": 8.769351,
     "end_time": "2025-07-26T15:21:29.877291",
     "exception": false,
     "start_time": "2025-07-26T15:21:21.107940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6703cc43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-26T15:21:29.905932Z",
     "iopub.status.busy": "2025-07-26T15:21:29.904764Z",
     "iopub.status.idle": "2025-07-26T15:21:30.084993Z",
     "shell.execute_reply": "2025-07-26T15:21:30.083848Z"
    },
    "papermill": {
     "duration": 0.195898,
     "end_time": "2025-07-26T15:21:30.086147",
     "exception": true,
     "start_time": "2025-07-26T15:21:29.890249",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/amex-data/final_train.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/3506038759.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/amex-data/final_train.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id4'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0mcheck_dtype_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mto_pandas_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"split_blocks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[0m\u001b[1;32m    268\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mfilesystem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# fsspec resources can also point to directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         handles = get_handle(\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/amex-data/final_train.parquet'"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_parquet('/kaggle/input/amex-data/final_train.parquet')\n",
    "print(train_df.shape)\n",
    "train_df['id4'] = pd.to_datetime(train_df['id4'])\n",
    "\n",
    "train_df\n",
    "gc.collect()\n",
    "train_df['id1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b09bc6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:45:06.939290Z",
     "iopub.status.busy": "2025-07-20T17:45:06.939074Z",
     "iopub.status.idle": "2025-07-20T17:45:26.820821Z",
     "shell.execute_reply": "2025-07-20T17:45:26.820229Z",
     "shell.execute_reply.started": "2025-07-20T17:45:06.939273Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_trans = pd.read_parquet('/kaggle/input/amex-data/add_trans.parquet')\n",
    "df_event = pd.read_parquet('/kaggle/input/amex-data/add_event.parquet')\n",
    "df_offer = pd.read_parquet('/kaggle/input/amex-data/offer_metadata.parquet')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b72cda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:45:26.821689Z",
     "iopub.status.busy": "2025-07-20T17:45:26.821486Z",
     "iopub.status.idle": "2025-07-20T17:45:26.835945Z",
     "shell.execute_reply": "2025-07-20T17:45:26.835345Z",
     "shell.execute_reply.started": "2025-07-20T17:45:26.821672Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(train_df['id2'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50476925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:45:26.837570Z",
     "iopub.status.busy": "2025-07-20T17:45:26.837353Z",
     "iopub.status.idle": "2025-07-20T17:45:30.006814Z",
     "shell.execute_reply": "2025-07-20T17:45:30.006041Z",
     "shell.execute_reply.started": "2025-07-20T17:45:26.837553Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_parquet('/kaggle/input/amex-data/final_test.parquet')\n",
    "print(test_df.shape)\n",
    "test_df.head()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850429c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:45:30.007624Z",
     "iopub.status.busy": "2025-07-20T17:45:30.007439Z",
     "iopub.status.idle": "2025-07-20T17:45:30.012621Z",
     "shell.execute_reply": "2025-07-20T17:45:30.011775Z",
     "shell.execute_reply.started": "2025-07-20T17:45:30.007609Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "set(df_offer) & set(df_event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ffc4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:45:30.013870Z",
     "iopub.status.busy": "2025-07-20T17:45:30.013508Z",
     "iopub.status.idle": "2025-07-20T17:45:30.072809Z",
     "shell.execute_reply": "2025-07-20T17:45:30.072218Z",
     "shell.execute_reply.started": "2025-07-20T17:45:30.013836Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def handle_missing_values(df: pd.DataFrame, missing_threshold: float = 0.95) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Handle missing values in a DataFrame\n",
    "    \n",
    "#     Parameters:\n",
    "#     - df: Input DataFrame\n",
    "#     - missing_threshold: Drop columns with missing ratio >= this threshold (default: 0.95)\n",
    "    \n",
    "#     Returns:\n",
    "#     - DataFrame with missing values handled\n",
    "#     \"\"\"\n",
    "#     df_copy = df.copy()\n",
    "    \n",
    "#     # Get all columns (excluding non-feature columns if needed)\n",
    "#     feature_cols = df_copy.columns.tolist()\n",
    "    \n",
    "#     # Track columns to drop\n",
    "#     cols_to_drop = []\n",
    "    \n",
    "#     # Check each column for missing values\n",
    "#     for col in feature_cols:\n",
    "#         missing_count = df_copy[col].isna().sum()\n",
    "#         missing_ratio = missing_count / len(df_copy)\n",
    "        \n",
    "#         # Drop columns with too many missing values\n",
    "#         if missing_ratio >= missing_threshold:\n",
    "#             cols_to_drop.append(col)\n",
    "#             continue\n",
    "        \n",
    "#         # Fill missing values based on data type\n",
    "#         if df_copy[col].dtype in ['float32', 'float64', 'int32', 'int64']:\n",
    "#             # For numeric columns, use median\n",
    "#             fill_value = df_copy[col].median()\n",
    "#             df_copy[col] = df_copy[col].fillna(fill_value)\n",
    "#         else:\n",
    "#             # For non-numeric columns, use mode or 0\n",
    "#             if df_copy[col].dtype == 'object':\n",
    "#                 # For categorical/string columns, use mode\n",
    "#                 mode_value = df_copy[col].mode()\n",
    "#                 if len(mode_value) > 0:\n",
    "#                     df_copy[col] = df_copy[col].fillna(mode_value[0])\n",
    "#                 else:\n",
    "#                     df_copy[col] = df_copy[col].fillna('unknown')\n",
    "#             else:\n",
    "#                 # For other types, use 0\n",
    "#                 df_copy[col] = df_copy[col].fillna(0)\n",
    "    \n",
    "#     # Drop columns with too many missing values\n",
    "#     if cols_to_drop:\n",
    "#         df_copy = df_copy.drop(columns=cols_to_drop)\n",
    "#         print(f\"Dropped {len(cols_to_drop)} columns with ≥{missing_threshold:.1%} missing values\")\n",
    "    \n",
    "#     return df_copy\n",
    "\n",
    "# train_df = handle_missing_values(train_df)\n",
    "# test_df = handle_missing_values(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745261fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:45:30.073872Z",
     "iopub.status.busy": "2025-07-20T17:45:30.073605Z",
     "iopub.status.idle": "2025-07-20T17:45:49.577645Z",
     "shell.execute_reply": "2025-07-20T17:45:49.577072Z",
     "shell.execute_reply.started": "2025-07-20T17:45:30.073839Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_missing_values_safe(train_df: pd.DataFrame, test_df: pd.DataFrame, \n",
    "                               missing_threshold: float = 0.95) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Handle missing values in train and test DataFrames, only dropping columns that exist in both\n",
    "    \n",
    "    Parameters:\n",
    "    - train_df: Training DataFrame\n",
    "    - test_df: Test DataFrame  \n",
    "    - missing_threshold: Drop columns with missing ratio >= this threshold (default: 0.95)\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of (train_df, test_df) with missing values handled\n",
    "    \"\"\"\n",
    "    train_copy = train_df.copy()\n",
    "    test_copy = test_df.copy()\n",
    "    \n",
    "    # Get columns that exist in both datasets\n",
    "    common_columns = set(train_copy.columns) & set(test_copy.columns)\n",
    "    \n",
    "    # Track columns to drop (only from common columns)\n",
    "    cols_to_drop = []\n",
    "    \n",
    "    # Check each common column for missing values\n",
    "    for col in common_columns:\n",
    "        # Check missing ratio in both train and test\n",
    "        train_missing_ratio = train_copy[col].isna().sum() / len(train_copy)\n",
    "        test_missing_ratio = test_copy[col].isna().sum() / len(test_copy)\n",
    "        \n",
    "        # Drop if either dataset has too many missing values\n",
    "        if train_missing_ratio >= missing_threshold or test_missing_ratio >= missing_threshold:\n",
    "            cols_to_drop.append(col)\n",
    "            print(f\"Column '{col}': train missing {train_missing_ratio:.1%}, test missing {test_missing_ratio:.1%}\")\n",
    "    \n",
    "    # Drop the identified columns from both datasets\n",
    "    if cols_to_drop:\n",
    "        train_copy = train_copy.drop(columns=cols_to_drop)\n",
    "        test_copy = test_copy.drop(columns=cols_to_drop)\n",
    "        print(f\"Dropped {len(cols_to_drop)} common columns with ≥{missing_threshold:.1%} missing values\")\n",
    "    \n",
    "    # Fill missing values in train dataset\n",
    "    for col in train_copy.columns:\n",
    "        missing_count = train_copy[col].isna().sum()\n",
    "        if missing_count == 0:\n",
    "            continue\n",
    "            \n",
    "        if train_copy[col].dtype in ['float32', 'float64', 'int32', 'int64']:\n",
    "            fill_value = train_copy[col].median()\n",
    "            train_copy[col] = train_copy[col].fillna(fill_value)\n",
    "        elif train_copy[col].dtype == 'object':\n",
    "            mode_value = train_copy[col].mode()\n",
    "            if len(mode_value) > 0:\n",
    "                train_copy[col] = train_copy[col].fillna(mode_value[0])\n",
    "            else:\n",
    "                train_copy[col] = train_copy[col].fillna('unknown')\n",
    "        else:\n",
    "            train_copy[col] = train_copy[col].fillna(0)\n",
    "    \n",
    "    # Fill missing values in test dataset\n",
    "    for col in test_copy.columns:\n",
    "        missing_count = test_copy[col].isna().sum()\n",
    "        if missing_count == 0:\n",
    "            continue\n",
    "            \n",
    "        if test_copy[col].dtype in ['float32', 'float64', 'int32', 'int64']:\n",
    "            fill_value = test_copy[col].median()\n",
    "            test_copy[col] = test_copy[col].fillna(fill_value)\n",
    "        elif test_copy[col].dtype == 'object':\n",
    "            mode_value = test_copy[col].mode()\n",
    "            if len(mode_value) > 0:\n",
    "                test_copy[col] = test_copy[col].fillna(mode_value[0])\n",
    "            else:\n",
    "                test_copy[col] = test_copy[col].fillna('unknown')\n",
    "        else:\n",
    "            test_copy[col] = test_copy[col].fillna(0)\n",
    "    \n",
    "    print(\"Missing value handling completed\")\n",
    "    return train_copy, test_copy\n",
    "\n",
    "train_df, test_df = handle_missing_values_safe(train_df, test_df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8168d0bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:45:49.578537Z",
     "iopub.status.busy": "2025-07-20T17:45:49.578312Z",
     "iopub.status.idle": "2025-07-20T17:45:49.582765Z",
     "shell.execute_reply": "2025-07-20T17:45:49.582012Z",
     "shell.execute_reply.started": "2025-07-20T17:45:49.578516Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def handle_missing_values(df: pd.DataFrame, missing_threshold: float = 0.95) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Handle missing values in a DataFrame\n",
    "    \n",
    "#     Parameters:\n",
    "#     - df: Input DataFrame\n",
    "#     - missing_threshold: Drop columns with missing ratio >= this threshold (default: 0.95)\n",
    "    \n",
    "#     Returns:\n",
    "#     - DataFrame with missing values handled\n",
    "#     \"\"\"\n",
    "#     df_copy = df.copy()\n",
    "    \n",
    "#     # Get all columns (excluding non-feature columns if needed)\n",
    "#     feature_cols = df_copy.columns.tolist()\n",
    "    \n",
    "#     # Track columns to drop\n",
    "#     cols_to_drop = []\n",
    "    \n",
    "#     # Check each column for missing values\n",
    "#     for col in feature_cols:\n",
    "#         missing_count = df_copy[col].isna().sum()\n",
    "#         missing_ratio = missing_count / len(df_copy)\n",
    "        \n",
    "#         # Drop columns with too many missing values\n",
    "#         if missing_ratio >= missing_threshold:\n",
    "#             cols_to_drop.append(col)\n",
    "#             continue\n",
    "        \n",
    "#         # Fill missing values based on data type\n",
    "#         if df_copy[col].dtype in ['float32', 'float64', 'int32', 'int64']:\n",
    "#             # For numeric columns, use median\n",
    "#             fill_value = df_copy[col].median()\n",
    "#             df_copy[col] = df_copy[col].fillna(fill_value)\n",
    "#         else:\n",
    "#             # For non-numeric columns, use mode or 0\n",
    "#             if df_copy[col].dtype == 'object':\n",
    "#                 # For categorical/string columns, use mode\n",
    "#                 mode_value = df_copy[col].mode()\n",
    "#                 if len(mode_value) > 0:\n",
    "#                     df_copy[col] = df_copy[col].fillna(mode_value[0])\n",
    "#                 else:\n",
    "#                     df_copy[col] = df_copy[col].fillna('unknown')\n",
    "#             else:\n",
    "#                 # For other types, use 0\n",
    "#                 df_copy[col] = df_copy[col].fillna(0)\n",
    "    \n",
    "#     # Drop columns with too many missing values\n",
    "#     if cols_to_drop:\n",
    "#         df_copy = df_copy.drop(columns=cols_to_drop)\n",
    "#         print(f\"Dropped {len(cols_to_drop)} columns with ≥{missing_threshold:.1%} missing values\")\n",
    "    \n",
    "#     return df_copy\n",
    "\n",
    "# df_event = handle_missing_values(df_event)\n",
    "# df_trans = handle_missing_values(df_trans)\n",
    "# df_offer = handle_missing_values(df_offer)\n",
    "# gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee851edd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T09:00:00.962272Z",
     "iopub.status.busy": "2025-07-25T09:00:00.961536Z",
     "iopub.status.idle": "2025-07-25T09:00:00.970798Z",
     "shell.execute_reply": "2025-07-25T09:00:00.970012Z",
     "shell.execute_reply.started": "2025-07-25T09:00:00.962251Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# def add_new_features(train_df: pd.DataFrame, test_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "#     \"\"\"\n",
    "#     Add new features to train and test DataFrames including interaction, customer-level aggregations,\n",
    "#     temporal, clustering, ratio, offer frequency, and cyclical encoding for timestamp features.\n",
    "#     Avoids using the target variable 'y' to prevent target leakage.\n",
    "\n",
    "#     Parameters:\n",
    "#     - train_df: Training DataFrame with 'id4' (datetime or convertible to datetime)\n",
    "#     - test_df: Test DataFrame with 'id4' (datetime or convertible to datetime)\n",
    "\n",
    "#     Returns:\n",
    "#     - Tuple of (train_df, test_df) with new features added\n",
    "#     \"\"\"\n",
    "#     # Create copies to avoid modifying original DataFrames\n",
    "#     train_df = train_df.copy()\n",
    "#     test_df = test_df.copy()\n",
    "\n",
    "#     # Check for required columns\n",
    "#     required_cols = ['id2', 'id3', 'id4', 'f132', 'best_day_ctr', 'f366', 'overall_ctr', 'performance_consistency', 'engagement_intensity']\n",
    "#     missing_cols_train = [col for col in required_cols + ['y'] if col not in train_df.columns]\n",
    "#     if missing_cols_train:\n",
    "#         raise KeyError(f\"Missing columns in train_df: {missing_cols_train}\")\n",
    "#     missing_cols_test = [col for col in required_cols if col not in test_df.columns]\n",
    "#     if missing_cols_test:\n",
    "#         raise KeyError(f\"Missing columns in test_df: {missing_cols_test}\")\n",
    "\n",
    "#     # Convert id4 to datetime and handle invalid/missing values\n",
    "#     print(\"Converting id4 to datetime...\")\n",
    "#     for df, name in [(train_df, 'train_df'), (test_df, 'test_df')]:\n",
    "#         try:\n",
    "#             df['id4'] = pd.to_datetime(df['id4'], errors='coerce')\n",
    "#             if df['id4'].isna().any():\n",
    "#                 print(f\"Warning: {name} has {df['id4'].isna().sum()} invalid id4 values. Filling with max timestamp.\")\n",
    "#                 df['id4'] = df['id4'].fillna(df['id4'].max())\n",
    "#             print(f\"{name} id4 dtype: {df['id4'].dtype}\")\n",
    "#         except Exception as e:\n",
    "#             raise ValueError(f\"Failed to convert id4 to datetime in {name}: {str(e)}\")\n",
    "\n",
    "#     # 1. Interaction Features\n",
    "#     print(\"Adding interaction features...\")\n",
    "#     for df, name in [(train_df, 'train_df'), (test_df, 'test_df')]:\n",
    "#         df['f132_best_day_ctr'] = df['f132'] * df['best_day_ctr']\n",
    "#         print(f\"Created feature for {name}: f132_best_day_ctr\")\n",
    "#         df['f366_over_overall_ctr'] = df['f366'] / (df['overall_ctr'] + 1e-10)\n",
    "#         print(f\"Created feature for {name}: f366_over_overall_ctr\")\n",
    "\n",
    "#     # 2. Customer-Level Aggregations\n",
    "#     print(\"Adding customer-level aggregation features...\")\n",
    "#     for df, name in [(train_df, 'train_df'), (test_df, 'test_df')]:\n",
    "#         df['mean_f132_per_id2'] = df.groupby('id2')['f132'].transform('mean')\n",
    "#         print(f\"Created feature for {name}: mean_f132_per_id2\")\n",
    "#         df['max_best_day_ctr_per_id2'] = df.groupby('id2')['best_day_ctr'].transform('max')\n",
    "#         print(f\"Created feature for {name}: max_best_day_ctr_per_id2\")\n",
    "\n",
    "#     # 3. Temporal Features\n",
    "#     print(\"Adding temporal features...\")\n",
    "#     for df, name in [(train_df, 'train_df'), (test_df, 'test_df')]:\n",
    "#         df['recency'] = df.groupby('id2')['id4'].transform(lambda x: (x.max() - x).dt.total_seconds() / 3600)\n",
    "#         print(f\"Created feature for {name}: recency\")\n",
    "\n",
    "#     # Additional temporal features\n",
    "#     for df, name in [(train_df, 'train_df'), (test_df, 'test_df')]:\n",
    "#         if df is not None:\n",
    "#             # Time since previous transaction in hours per customer\n",
    "#             df['time_since_prev_txn_hours'] = df.groupby('id2')['id4'].diff().dt.total_seconds() / 3600.0\n",
    "#             df['time_since_prev_txn_hours'] = df['time_since_prev_txn_hours'].fillna(df['time_since_prev_txn_hours'].max())\n",
    "#             print(f\"Created feature for {name}: time_since_prev_txn_hours\")\n",
    "            \n",
    "#             # Rolling average of f132 (3-transaction window)\n",
    "#             df['f132_rolling_mean_3'] = df.groupby('id2')['f132'].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
    "#             print(f\"Created feature for {name}: f132_rolling_mean_3\")\n",
    "            \n",
    "#             # Standard deviation of best_day_ctr per customer\n",
    "#             df['best_day_ctr_std_per_id2'] = df.groupby('id2')['best_day_ctr'].transform('std').fillna(0)\n",
    "#             print(f\"Created feature for {name}: best_day_ctr_std_per_id2\")\n",
    "\n",
    "#     # Non-optimized rolling count function (reverted to original)\n",
    "#     def add_rolling_counts(df, name, hours):\n",
    "#         \"\"\"Add rolling count of transactions for specified hours\"\"\"\n",
    "#         df[f'transaction_count_last_{hours}h'] = 0\n",
    "#         for id2 in df['id2'].unique():\n",
    "#             customer_mask = df['id2'] == id2\n",
    "#             customer_data = df[customer_mask].copy()\n",
    "#             customer_data = customer_data.sort_values('id4')\n",
    "#             rolling_counts = []\n",
    "#             for current_time in customer_data['id4']:\n",
    "#                 time_window = current_time - pd.Timedelta(hours=hours)\n",
    "#                 count = ((customer_data['id4'] >= time_window) & (customer_data['id4'] <= current_time)).sum()\n",
    "#                 rolling_counts.append(count)\n",
    "#             df.loc[customer_mask, f'transaction_count_last_{hours}h'] = rolling_counts\n",
    "#         # Debugging: Log summary statistics for rolling counts\n",
    "#         print(f\"{name} transaction_count_last_{hours}h stats: mean={df[f'transaction_count_last_{hours}h'].mean():.2f}, \"\n",
    "#               f\"min={df[f'transaction_count_last_{hours}h'].min()}, \"\n",
    "#               f\"max={df[f'transaction_count_last_{hours}h'].max()}\")\n",
    "#         return df\n",
    "\n",
    "#     # Apply rolling counts for 6h, 12h, 24h\n",
    "#     for df, name in [(train_df, 'train_df'), (test_df, 'test_df')]:\n",
    "#         df = add_rolling_counts(df, name, 6)\n",
    "#         df = add_rolling_counts(df, name, 12)\n",
    "#         df = add_rolling_counts(df, name, 24)\n",
    "\n",
    "#     def time_since_last_event(df, name):\n",
    "#         df['time_since_last_event'] = df.groupby('id2')['id4'].diff().shift(-1).dt.total_seconds() / 3600\n",
    "#         df['time_since_last_event'] = df['time_since_last_event'].fillna(df['time_since_last_event'].max())\n",
    "#         print(f\"Created feature for {name}: time_since_last_event\")\n",
    "#         # Debugging: Log summary statistics\n",
    "#         print(f\"{name} time_since_last_event stats: mean={df['time_since_last_event'].mean():.2f}, \"\n",
    "#               f\"min={df['time_since_last_event'].min():.2f}, \"\n",
    "#               f\"max={df['time_since_last_event'].max():.2f}\")\n",
    "#         return df\n",
    "\n",
    "#     train_df = time_since_last_event(train_df, 'train_df')\n",
    "#     test_df = time_since_last_event(test_df, 'test_df')\n",
    "\n",
    "#     # 4. Cyclical Encoding for Timestamp Features\n",
    "#     print(\"Adding cyclical encoding for timestamp features...\")\n",
    "#     def cyclical_encode(df, name, column, period):\n",
    "#         df[f'{column}_sin'] = np.sin(2 * np.pi * df[column] / period)\n",
    "#         df[f'{column}_cos'] = np.cos(2 * np.pi * df[column] / period)\n",
    "#         print(f\"Created features for {name}: {column}_sin, {column}_cos\")\n",
    "#         return df\n",
    "\n",
    "#     # Extract time components from id4\n",
    "#     for df, name in [(train_df, 'train_df'), (test_df, 'test_df')]:\n",
    "#         df['hour'] = df['id4'].dt.hour\n",
    "#         df['day_of_week'] = df['id4'].dt.dayofweek\n",
    "#         df['month'] = df['id4'].dt.month\n",
    "#         # Apply cyclical encoding\n",
    "#         df = cyclical_encode(df, name, 'hour', 24)\n",
    "#         df = cyclical_encode(df, name, 'day_of_week', 7)\n",
    "#         df = cyclical_encode(df, name, 'month', 12)\n",
    "#         # Drop original time components to avoid redundancy\n",
    "#         df = df.drop(['hour', 'day_of_week', 'month'], axis=1)\n",
    "\n",
    "#     # 5. Feature Clustering\n",
    "#     print(\"Adding clustering features...\")\n",
    "#     cluster_features = ['f132', 'best_day_ctr']\n",
    "#     scaler = StandardScaler()\n",
    "#     cluster_data = scaler.fit_transform(train_df[cluster_features].fillna(0))\n",
    "#     kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "#     train_df['customer_cluster'] = kmeans.fit_predict(cluster_data)\n",
    "#     print(\"Created feature for train_df: customer_cluster\")\n",
    "#     test_df['customer_cluster'] = kmeans.predict(scaler.transform(test_df[cluster_features].fillna(0)))\n",
    "#     print(\"Created feature for test_df: customer_cluster\")\n",
    "\n",
    "#     # 6. Ratio Features\n",
    "#     print(\"Adding ratio features...\")\n",
    "#     for df, name in [(train_df, 'train_df'), (test_df, 'test_df')]:\n",
    "#         df['perf_consistency_over_engage'] = df['performance_consistency'] / (df['engagement_intensity'] + 1e-10)\n",
    "#         print(f\"Created feature for {name}: perf_consistency_over_engage\")\n",
    "\n",
    "#     # 7. Offer Frequency (replacing offer_click_rate to avoid target leakage)\n",
    "#     print(\"Adding offer frequency feature...\")\n",
    "#     offer_frequency = train_df.groupby('id3').size().to_dict()\n",
    "#     train_df['offer_frequency'] = train_df['id3'].map(offer_frequency)\n",
    "#     print(\"Created feature for train_df: offer_frequency\")\n",
    "#     test_df['offer_frequency'] = test_df['id3'].map(offer_frequency)\n",
    "#     print(\"Created feature for test_df: offer_frequency\")\n",
    "\n",
    "#     # 8. New Temporal Features\n",
    "#     print(\"Adding new temporal features...\")\n",
    "#     for df, name in [(train_df, 'train_df'), (test_df, 'test_df')]:\n",
    "#         # 1. Log-transform of time_since_last_event\n",
    "#         df['log_time_since_last_event'] = np.log1p(df['time_since_last_event'].clip(lower=0))\n",
    "#         print(f\"Created feature for {name}: log_time_since_last_event\")\n",
    "        \n",
    "#         # 2. Time since first event per id2 (in hours)\n",
    "#         first_event = df.groupby('id2')['id4'].transform('min')\n",
    "#         df['time_since_first_event_hours'] = (df['id4'] - first_event).dt.total_seconds() / 3600.0\n",
    "#         df['time_since_first_event_hours'] = df['time_since_first_event_hours'].fillna(df['time_since_first_event_hours'].max())\n",
    "#         print(f\"Created feature for {name}: time_since_first_event_hours\")\n",
    "        \n",
    "#         # 5. Hour of day (non-cyclical, for compatibility with existing features)\n",
    "#         df['hour_of_day'] = df['id4'].dt.hour\n",
    "#         print(f\"Created feature for {name}: hour_of_day\")\n",
    "        \n",
    "#         # 6. Day of week (non-cyclical, for compatibility)\n",
    "#         df['day_of_week'] = df['id4'].dt.dayofweek\n",
    "#         print(f\"Created feature for {name}: day_of_week\")\n",
    "        \n",
    "#         # 7. Rolling std of f132 over last 3 transactions\n",
    "#         df['f132_rolling_std_3'] = df.groupby('id2')['f132'].transform(\n",
    "#             lambda x: x.rolling(window=3, min_periods=1).std()\n",
    "#         ).fillna(0)\n",
    "#         print(f\"Created feature for {name}: f132_rolling_std_3\")\n",
    "        \n",
    "#         # 8. Ratio of f132_best_day_ctr to mean_f132_per_id2\n",
    "#         df['f132_best_day_ctr_ratio'] = df['f132_best_day_ctr'] / (df['mean_f132_per_id2'] + 1e-10)\n",
    "#         print(f\"Created feature for {name}: f132_best_day_ctr_ratio\")\n",
    "        \n",
    "#         # 9. Difference between f210 and f366\n",
    "#         if 'f210' in df.columns:\n",
    "#             df['f210_f366_diff'] = df['f210'] - df['f366']\n",
    "#         else:\n",
    "#             df['f210_f366_diff'] = 0  # Placeholder if f210 is missing\n",
    "#         print(f\"Created feature for {name}: f210_f366_diff\")\n",
    "        \n",
    "#         # 10. Transaction velocity (transactions per hour)\n",
    "#         df['transaction_velocity_12h'] = df['transaction_count_last_12h'] / (df['time_since_prev_txn_hours'] + 1e-10)\n",
    "#         print(f\"Created feature for {name}: transaction_velocity_12h\")\n",
    "#         # Debugging: Log summary statistics\n",
    "#         print(f\"{name} transaction_velocity_12h stats: mean={df['transaction_velocity_12h'].mean():.2f}, \"\n",
    "#               f\"min={df['transaction_velocity_12h'].min():.2f}, \"\n",
    "#               f\"max={df['transaction_velocity_12h'].max():.2f}\")\n",
    "\n",
    "#     # List of new features\n",
    "#     new_features = [\n",
    "#         'f132_best_day_ctr', 'f366_over_overall_ctr', 'mean_f132_per_id2',\n",
    "#         'max_best_day_ctr_per_id2', 'recency', 'time_since_prev_txn_hours',\n",
    "#         'f132_rolling_mean_3', 'best_day_ctr_std_per_id2', 'transaction_count_last_12h',\n",
    "#         'time_since_last_event', 'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos',\n",
    "#         'month_sin', 'month_cos', 'customer_cluster', 'perf_consistency_over_engage',\n",
    "#         'offer_frequency', 'log_time_since_last_event', 'time_since_first_event_hours',\n",
    "#         'transaction_count_last_6h', 'transaction_count_last_24h', 'hour_of_day',\n",
    "#         'day_of_week', 'f132_rolling_std_3', 'f132_best_day_ctr_ratio',\n",
    "#         'f210_f366_diff', 'transaction_velocity_12h'\n",
    "#     ]\n",
    "#     print(\"Feature engineering completed. Added features:\")\n",
    "#     print(new_features)\n",
    "\n",
    "#     return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcaa713",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:45:49.602956Z",
     "iopub.status.busy": "2025-07-20T17:45:49.602763Z",
     "iopub.status.idle": "2025-07-20T17:45:53.559405Z",
     "shell.execute_reply": "2025-07-20T17:45:53.558787Z",
     "shell.execute_reply.started": "2025-07-20T17:45:49.602942Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = train_df.sort_values('id4', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb03ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:45:53.560313Z",
     "iopub.status.busy": "2025-07-20T17:45:53.560119Z",
     "iopub.status.idle": "2025-07-20T17:45:56.018372Z",
     "shell.execute_reply": "2025-07-20T17:45:56.017667Z",
     "shell.execute_reply.started": "2025-07-20T17:45:53.560297Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_val_split(df: pd.DataFrame, val_ratio: float = 0.3) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split sorted DataFrame into train and validation sets\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Input sorted DataFrame\n",
    "    - val_ratio: Ratio of data to use for validation (default: 0.2 for 20%)\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of (train_df, val_df)\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    train_size = int(total_rows * (1 - val_ratio))\n",
    "    \n",
    "    # Top 80% for training\n",
    "    train_df = df.iloc[:train_size].copy()\n",
    "    \n",
    "    # Bottom 20% for validation\n",
    "    val_df = df.iloc[train_size:].copy()\n",
    "    \n",
    "    print(f\"Total rows: {total_rows:,}\")\n",
    "    print(f\"Train rows: {len(train_df):,} ({len(train_df)/total_rows:.1%})\")\n",
    "    print(f\"Val rows: {len(val_df):,} ({len(val_df)/total_rows:.1%})\")\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "train_df, val_df = train_val_split(train_df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4def6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:45:56.019289Z",
     "iopub.status.busy": "2025-07-20T17:45:56.019088Z",
     "iopub.status.idle": "2025-07-20T17:45:56.047019Z",
     "shell.execute_reply": "2025-07-20T17:45:56.046143Z",
     "shell.execute_reply.started": "2025-07-20T17:45:56.019273Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd8abf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:45:56.048400Z",
     "iopub.status.busy": "2025-07-20T17:45:56.047814Z",
     "iopub.status.idle": "2025-07-20T17:45:56.055091Z",
     "shell.execute_reply": "2025-07-20T17:45:56.054475Z",
     "shell.execute_reply.started": "2025-07-20T17:45:56.048377Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(val_df['id2'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdf81f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:45:56.056283Z",
     "iopub.status.busy": "2025-07-20T17:45:56.055939Z",
     "iopub.status.idle": "2025-07-20T17:45:56.062922Z",
     "shell.execute_reply": "2025-07-20T17:45:56.062323Z",
     "shell.execute_reply.started": "2025-07-20T17:45:56.056265Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exclude_cols = ['id1', 'id3', 'id4', 'id5', 'id6', 'id7', 'id8', \n",
    "#                    'id9', 'id10', 'id11', 'id12', 'id13', 'pred']\n",
    "# exclude_cols.extend([col for col in train_df.columns if col.endswith('_dt')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883c3aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:45:56.063940Z",
     "iopub.status.busy": "2025-07-20T17:45:56.063701Z",
     "iopub.status.idle": "2025-07-20T17:45:56.074723Z",
     "shell.execute_reply": "2025-07-20T17:45:56.074124Z",
     "shell.execute_reply.started": "2025-07-20T17:45:56.063924Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def remove_excluded_columns(train_df: pd.DataFrame, val_df: pd.DataFrame, \n",
    "#                           exclude_cols: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "#     \"\"\"\n",
    "#     Remove excluded columns from train and validation DataFrames\n",
    "    \n",
    "#     Parameters:\n",
    "#     - train_df: Training DataFrame\n",
    "#     - val_df: Validation DataFrame  \n",
    "#     - exclude_cols: List of column names to exclude\n",
    "    \n",
    "#     Returns:\n",
    "#     - Tuple of (cleaned_train_df, cleaned_val_df)\n",
    "#     \"\"\"\n",
    "#     # Find columns that actually exist in the DataFrames\n",
    "#     train_cols_to_drop = [col for col in exclude_cols if col in train_df.columns]\n",
    "#     val_cols_to_drop = [col for col in exclude_cols if col in val_df.columns]\n",
    "    \n",
    "#     # Drop columns\n",
    "#     train_df_clean = train_df.drop(columns=train_cols_to_drop)\n",
    "#     val_df_clean = val_df.drop(columns=val_cols_to_drop)\n",
    "    \n",
    "#     print(f\"Dropped {len(train_cols_to_drop)} columns from train_df\")\n",
    "#     print(f\"Dropped {len(val_cols_to_drop)} columns from val_df\")\n",
    "    \n",
    "#     if train_cols_to_drop:\n",
    "#         print(f\"Columns dropped from train: {train_cols_to_drop}\")\n",
    "#     if val_cols_to_drop:\n",
    "#         print(f\"Columns dropped from val: {val_cols_to_drop}\")\n",
    "    \n",
    "#     return train_df_clean, val_df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e6871",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:45:56.075587Z",
     "iopub.status.busy": "2025-07-20T17:45:56.075406Z",
     "iopub.status.idle": "2025-07-20T17:45:57.193744Z",
     "shell.execute_reply": "2025-07-20T17:45:57.193209Z",
     "shell.execute_reply.started": "2025-07-20T17:45:56.075574Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from numba import njit\n",
    "import numpy as np\n",
    "\n",
    "# Numba function for rolling count in 12-hour window\n",
    "\n",
    "@njit\n",
    "def rolling_count_numba(timestamps_seconds, window_size_hours=12):\n",
    "    n = len(timestamps_seconds)\n",
    "    result = np.zeros(n, dtype=np.int32)\n",
    "    window_size_seconds = window_size_hours * 3600.0\n",
    "\n",
    "    for i in range(n):\n",
    "        current_time = timestamps_seconds[i]\n",
    "        count = 0\n",
    "        for j in range(n):\n",
    "            if timestamps_seconds[j] >= current_time - window_size_seconds and timestamps_seconds[j] <= current_time:\n",
    "                count += 1\n",
    "        result[i] = count\n",
    "    return result\n",
    "\n",
    "def rolling_count_12h_numba(group):\n",
    "    group_sorted = group.sort_values('id4')\n",
    "    timestamps = group_sorted['id4'].astype(np.int64).to_numpy() / 1e9  # Fix: convert to NumPy array\n",
    "    counts = rolling_count_numba(timestamps)\n",
    "    return pd.Series(counts, index=group_sorted.index)\n",
    "\n",
    "\n",
    "\n",
    "def add_new_features(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Add new features to train, validation, and test DataFrames including interaction, customer-level aggregations,\n",
    "    temporal, clustering, ratio, offer frequency, and cyclical encoding for timestamp features.\n",
    "    Avoids using the target variable 'y' to prevent target leakage.\n",
    "\n",
    "    Parameters:\n",
    "    - train_df: Training DataFrame with 'id4' (datetime or convertible to datetime)\n",
    "    - val_df: Validation DataFrame with 'id4' (datetime or convertible to datetime)\n",
    "    - test_df: Test DataFrame with 'id4' (datetime or convertible to datetime)\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of (train_df, val_df, test_df) with new features added\n",
    "    \"\"\"\n",
    "    # Create copies to avoid modifying original DataFrames\n",
    "    train_df = train_df.copy()\n",
    "    val_df = val_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "\n",
    "    # Check for required columns\n",
    "    required_cols = ['id2', 'id3', 'id4', 'f132', 'best_day_ctr', 'f366', 'overall_ctr', 'performance_consistency', 'engagement_intensity']\n",
    "    # Training and validation require 'y', but test may not\n",
    "    for df, name in [(train_df, 'train_df'), (val_df, 'val_df')]:\n",
    "        missing_cols = [col for col in required_cols + ['y'] if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise KeyError(f\"Missing columns in {name}: {missing_cols}\")\n",
    "    # Test DataFrame may not have 'y'\n",
    "    missing_cols_test = [col for col in required_cols if col not in test_df.columns]\n",
    "    if missing_cols_test:\n",
    "        raise KeyError(f\"Missing columns in test_df: {missing_cols_test}\")\n",
    "\n",
    "    # Convert id4 to datetime and handle invalid/missing values\n",
    "    print(\"Converting id4 to datetime...\")\n",
    "    for df, name in [(train_df, 'train_df'), (val_df, 'val_df'), (test_df, 'test_df')]:\n",
    "        try:\n",
    "            df['id4'] = pd.to_datetime(df['id4'], errors='coerce')\n",
    "            if df['id4'].isna().any():\n",
    "                print(f\"Warning: {name} has {df['id4'].isna().sum()} invalid id4 values. Filling with max timestamp.\")\n",
    "                df['id4'] = df['id4'].fillna(df['id4'].max())\n",
    "            print(f\"{name} id4 dtype: {df['id4'].dtype}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to convert id4 to datetime in {name}: {str(e)}\")\n",
    "\n",
    "    # 1. Interaction Features\n",
    "    print(\"Adding interaction features...\")\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        df['f132_best_day_ctr'] = df['f132'] * df['best_day_ctr']\n",
    "        df['f366_over_overall_ctr'] = df['f366'] / (df['overall_ctr'] + 1e-10)\n",
    "\n",
    "    # 2. Customer-Level Aggregations\n",
    "    print(\"Adding customer-level aggregation features...\")\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        df['mean_f132_per_id2'] = df.groupby('id2')['f132'].transform('mean')\n",
    "        df['max_best_day_ctr_per_id2'] = df.groupby('id2')['best_day_ctr'].transform('max')\n",
    "\n",
    "    # 3. Temporal Features\n",
    "    print(\"Adding temporal features...\")\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        df['recency'] = df.groupby('id2')['id4'].transform(lambda x: (x.max() - x).dt.total_seconds() / 3600)\n",
    "\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        if df is not None:\n",
    "            df_sorted = df.sort_values(['id2', 'id4'])\n",
    "            \n",
    "            # Time since previous transaction\n",
    "            df['time_since_prev_txn_hours'] = df_sorted.groupby('id2')['id4'].diff().dt.total_seconds() / 3600.0\n",
    "            df['time_since_prev_txn_hours'] = df['time_since_prev_txn_hours'].fillna(df['time_since_prev_txn_hours'].max())\n",
    "            \n",
    "            # Rolling average of f132\n",
    "            df['f132_rolling_mean_3'] = df_sorted.groupby('id2')['f132'].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
    "            \n",
    "            # Replace slow rolling count with Numba version\n",
    "            df['transaction_count_last_12h'] = df.groupby('id2').apply(rolling_count_12h_numba).reset_index(level=0, drop=True).reindex(df.index)\n",
    "            \n",
    "            # Std deviation of best_day_ctr\n",
    "            df['best_day_ctr_std_per_id2'] = df.groupby('id2')['best_day_ctr'].transform('std').fillna(0)\n",
    "\n",
    "\n",
    "    def time_since_last_event(df):\n",
    "        df = df.sort_values(['id2', 'id4'])\n",
    "        df['time_since_last_event'] = df.groupby('id2')['id4'].diff().shift(-1).dt.total_seconds() / 3600\n",
    "        df['time_since_last_event'] = df['time_since_last_event'].fillna(df['time_since_last_event'].max())\n",
    "        return df\n",
    "\n",
    "    train_df = time_since_last_event(train_df)\n",
    "    val_df = time_since_last_event(val_df)\n",
    "    test_df = time_since_last_event(test_df)\n",
    "\n",
    "    # 4. Cyclical Encoding for Timestamp Features\n",
    "    print(\"Adding cyclical encoding for timestamp features...\")\n",
    "    def cyclical_encode(df, column, period):\n",
    "        df[f'{column}_sin'] = np.sin(2 * np.pi * df[column] / period)\n",
    "        df[f'{column}_cos'] = np.cos(2 * np.pi * df[column] / period)\n",
    "        return df\n",
    "\n",
    "    # Extract time components from id4\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        df['hour'] = df['id4'].dt.hour\n",
    "        df['day_of_week'] = df['id4'].dt.dayofweek\n",
    "        df['month'] = df['id4'].dt.month\n",
    "        # Apply cyclical encoding\n",
    "        df = cyclical_encode(df, 'hour', 24)\n",
    "        df = cyclical_encode(df, 'day_of_week', 7)\n",
    "        df = cyclical_encode(df, 'month', 12)\n",
    "        # Drop original time components to avoid redundancy\n",
    "        df = df.drop(['hour', 'day_of_week', 'month'], axis=1)\n",
    "\n",
    "    # 5. Feature Clustering\n",
    "    print(\"Adding clustering features...\")\n",
    "    cluster_features = ['f132', 'best_day_ctr']\n",
    "    scaler = StandardScaler()\n",
    "    cluster_data = scaler.fit_transform(train_df[cluster_features].fillna(0))\n",
    "    kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "    train_df['customer_cluster'] = kmeans.fit_predict(cluster_data)\n",
    "    val_df['customer_cluster'] = kmeans.predict(scaler.transform(val_df[cluster_features].fillna(0)))\n",
    "    test_df['customer_cluster'] = kmeans.predict(scaler.transform(test_df[cluster_features].fillna(0)))\n",
    "\n",
    "    # 6. Ratio Features\n",
    "    print(\"Adding ratio features...\")\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        df['perf_consistency_over_engage'] = df['performance_consistency'] / (df['engagement_intensity'] + 1e-10)\n",
    "\n",
    "    # 7. Offer Frequency (replacing offer_click_rate to avoid target leakage)\n",
    "    print(\"Adding offer frequency feature...\")\n",
    "    offer_frequency = train_df.groupby('id3').size().to_dict()\n",
    "    train_df['offer_frequency'] = train_df['id3'].map(offer_frequency)\n",
    "    val_df['offer_frequency'] = val_df['id3'].map(offer_frequency)\n",
    "    test_df['offer_frequency'] = test_df['id3'].map(offer_frequency)\n",
    "\n",
    "    # List of new features\n",
    "    new_features = [\n",
    "        'f132_best_day_ctr', 'f366_over_overall_ctr', 'mean_f132_per_id2',\n",
    "        'max_best_day_ctr_per_id2', 'recency', 'time_since_prev_txn_hours',\n",
    "        'f132_rolling_mean_3', 'transaction_count_last_12h', 'best_day_ctr_std_per_id2',\n",
    "        'time_since_last_event', 'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos',\n",
    "        'month_sin', 'month_cos', 'customer_cluster', 'perf_consistency_over_engage',\n",
    "        'offer_frequency'\n",
    "    ]\n",
    "    print(\"Feature engineering completed. Added features:\")\n",
    "    print(new_features)\n",
    "\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ef302",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:45:57.194561Z",
     "iopub.status.busy": "2025-07-20T17:45:57.194393Z",
     "iopub.status.idle": "2025-07-20T17:45:57.199412Z",
     "shell.execute_reply": "2025-07-20T17:45:57.198650Z",
     "shell.execute_reply.started": "2025-07-20T17:45:57.194548Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_event.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63322d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:45:57.200607Z",
     "iopub.status.busy": "2025-07-20T17:45:57.200367Z",
     "iopub.status.idle": "2025-07-20T17:46:01.347760Z",
     "shell.execute_reply": "2025-07-20T17:46:01.347215Z",
     "shell.execute_reply.started": "2025-07-20T17:45:57.200590Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import cudf\n",
    "from cuml.preprocessing import StandardScaler as cuStandardScaler\n",
    "from cuml.cluster import KMeans as cuKMeans\n",
    "from cuml.decomposition import PCA as cuPCA\n",
    "from scipy.stats import entropy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from numba import cuda, jit, prange\n",
    "import dask_cudf\n",
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "# ==========================================\n",
    "# GPU UTILITIES\n",
    "# ==========================================\n",
    "\n",
    "def to_cudf(df):\n",
    "    \"\"\"Convert pandas DataFrame to cuDF DataFrame\"\"\"\n",
    "    return cudf.from_pandas(df)\n",
    "\n",
    "def to_pandas(gdf):\n",
    "    \"\"\"Convert cuDF DataFrame back to pandas\"\"\"\n",
    "    return gdf.to_pandas()\n",
    "\n",
    "@cuda.jit\n",
    "def calculate_ctr_kernel(impressions, clicks, ctr_out):\n",
    "    \"\"\"CUDA kernel for CTR calculation\"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < impressions.size:\n",
    "        ctr_out[idx] = clicks[idx] / (impressions[idx] + 1e-6)\n",
    "\n",
    "# ==========================================\n",
    "# 1. GPU-OPTIMIZED BEHAVIORAL PROXIES\n",
    "# ==========================================\n",
    "\n",
    "def identify_overlap_customers(df_event, df_trans):\n",
    "    \"\"\"GPU-accelerated customer overlap identification\"\"\"\n",
    "    # Convert to GPU\n",
    "    gdf_event = to_cudf(df_event[['id2']].drop_duplicates())\n",
    "    gdf_trans = to_cudf(df_trans[['id2']].drop_duplicates())\n",
    "    \n",
    "    # GPU merge to find overlaps\n",
    "    overlap = gdf_event.merge(gdf_trans, on='id2', how='inner')\n",
    "    overlap_customers = overlap['id2'].to_pandas().tolist()\n",
    "    \n",
    "    print(f\"Found {len(overlap_customers)} overlapping customers\")\n",
    "    return overlap_customers\n",
    "\n",
    "def calculate_same_day_activity_gpu(gdf_trans, gdf_event, profiles):\n",
    "    \"\"\"GPU-accelerated same-day activity calculation\"\"\"\n",
    "    # Extract dates\n",
    "    gdf_trans['trans_date'] = cudf.to_datetime(gdf_trans['f370']).dt.floor('D')\n",
    "    gdf_event['event_date'] = cudf.to_datetime(gdf_event['id4']).dt.floor('D')\n",
    "    \n",
    "    # Count unique dates per customer\n",
    "    trans_dates = gdf_trans.groupby('id2')['trans_date'].nunique().reset_index()\n",
    "    trans_dates.columns = ['id2', 'trans_unique_days']\n",
    "    \n",
    "    event_dates = gdf_event.groupby('id2')['event_date'].nunique().reset_index()\n",
    "    event_dates.columns = ['id2', 'event_unique_days']\n",
    "    \n",
    "    # Merge dates for same-day detection\n",
    "    merged_dates = gdf_trans[['id2', 'trans_date']].merge(\n",
    "        gdf_event[['id2', 'event_date']], \n",
    "        left_on=['id2', 'trans_date'], \n",
    "        right_on=['id2', 'event_date'],\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    same_day_counts = merged_dates.groupby('id2').size().reset_index()\n",
    "    same_day_counts.columns = ['id2', 'same_day_count']\n",
    "    \n",
    "    # Add to profiles\n",
    "    profiles = profiles.merge(trans_dates, on='id2', how='left')\n",
    "    profiles = profiles.merge(event_dates, on='id2', how='left')\n",
    "    profiles = profiles.merge(same_day_counts, on='id2', how='left')\n",
    "    \n",
    "    # Calculate ratio\n",
    "    total_unique_days = profiles['trans_unique_days'] + profiles['event_unique_days'] - profiles['same_day_count']\n",
    "    profiles['same_day_activity_ratio'] = profiles['same_day_count'] / (total_unique_days + 1e-6)\n",
    "    \n",
    "    return profiles\n",
    "\n",
    "def create_customer_profiles_gpu(df_event, df_trans, overlap_customers):\n",
    "    \"\"\"Vectorized GPU customer profiling\"\"\"\n",
    "    print(\"Creating GPU-accelerated customer profiles...\")\n",
    "    \n",
    "    # Convert to GPU DataFrames\n",
    "    gdf_trans = to_cudf(df_trans[df_trans['id2'].isin(overlap_customers)])\n",
    "    gdf_event = to_cudf(df_event[df_event['id2'].isin(overlap_customers)])\n",
    "    \n",
    "    # Transaction aggregations on GPU\n",
    "    trans_agg = gdf_trans.groupby('id2').agg({\n",
    "        'f367': ['mean', 'count', 'std', 'sum']\n",
    "    }).reset_index()\n",
    "    trans_agg.columns = ['id2', 'avg_transaction_amount', 'transaction_count', \n",
    "                        'transaction_std', 'total_trans_amount']\n",
    "    \n",
    "    # Event aggregations on GPU\n",
    "    event_agg = gdf_event.groupby('id2').agg({\n",
    "        'id3': 'count',\n",
    "        'id7': 'count'\n",
    "    }).reset_index()\n",
    "    event_agg.columns = ['id2', 'impression_count', 'click_count']\n",
    "    \n",
    "    # Unique offers count\n",
    "    unique_offers = gdf_event.groupby('id2')['id3'].nunique().reset_index()\n",
    "    unique_offers.columns = ['id2', 'unique_offers_seen']\n",
    "    \n",
    "    # Merge all on GPU\n",
    "    profiles = trans_agg.merge(event_agg, on='id2', how='outer')\n",
    "    profiles = profiles.merge(unique_offers, on='id2', how='left')\n",
    "    \n",
    "    # Calculate derived features on GPU\n",
    "    profiles['transaction_cv'] = profiles['transaction_std'] / (profiles['avg_transaction_amount'] + 1e-6)\n",
    "    profiles['ctr'] = profiles['click_count'] / (profiles['impression_count'] + 1e-6)\n",
    "    \n",
    "    # Calculate same-day activity using GPU operations\n",
    "    profiles = calculate_same_day_activity_gpu(gdf_trans, gdf_event, profiles)\n",
    "    \n",
    "    # Fill NaNs and convert back to pandas\n",
    "    profiles = profiles.fillna(0)\n",
    "    return profiles.to_pandas()\n",
    "\n",
    "def create_behavioral_clusters_gpu(profiles_df, n_clusters=25):\n",
    "    \"\"\"GPU-accelerated clustering\"\"\"\n",
    "    cluster_features = [\n",
    "        'avg_transaction_amount', 'transaction_count', 'transaction_cv',\n",
    "        'impression_count', 'ctr', 'same_day_activity_ratio'\n",
    "    ]\n",
    "    \n",
    "    # Convert to GPU\n",
    "    X = profiles_df[cluster_features].fillna(0).values\n",
    "    X_gpu = cp.asarray(X)\n",
    "    \n",
    "    # Scale on GPU\n",
    "    scaler = cuStandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_gpu)\n",
    "    \n",
    "    # GPU KMeans\n",
    "    kmeans = cuKMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        random_state=42,\n",
    "        n_init=10,\n",
    "        max_iter=300\n",
    "    )\n",
    "    \n",
    "    # Fit and predict\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    profiles_df['behavior_cluster'] = cp.asnumpy(labels)\n",
    "    \n",
    "    # Calculate cluster profiles\n",
    "    cluster_profiles = profiles_df.groupby('behavior_cluster')[cluster_features].mean()\n",
    "    \n",
    "    # Store profiles with index for fast lookup\n",
    "    customer_clusters = profiles_df.set_index('id2')['behavior_cluster'].to_dict()\n",
    "    \n",
    "    return profiles_df, cluster_profiles, customer_clusters, scaler, kmeans\n",
    "\n",
    "def batch_assign_clusters(customer_features_df, scaler, kmeans):\n",
    "    \"\"\"Batch assign clusters using GPU\"\"\"\n",
    "    feature_cols = ['avg_transaction_amount', 'transaction_count', 'transaction_cv',\n",
    "                   'impression_count', 'ctr', 'same_day_activity_ratio']\n",
    "    \n",
    "    X = customer_features_df[feature_cols].fillna(0).values\n",
    "    X_gpu = cp.asarray(X)\n",
    "    X_scaled = scaler.transform(X_gpu)\n",
    "    \n",
    "    labels = kmeans.predict(X_scaled)\n",
    "    return cp.asnumpy(labels)\n",
    "\n",
    "# ==========================================\n",
    "# 2. GPU-OPTIMIZED TEMPORAL FEATURES\n",
    "# ==========================================\n",
    "\n",
    "def define_holidays():\n",
    "    \"\"\"Define major shopping holidays\"\"\"\n",
    "    return {\n",
    "        '2023-11-24': 'black_friday',\n",
    "        '2023-11-27': 'cyber_monday',\n",
    "        '2023-12-25': 'christmas',\n",
    "        '2023-10-31': 'halloween',\n",
    "        '2023-11-23': 'thanksgiving',\n",
    "        '2023-01-01': 'new_year',\n",
    "        '2023-11-11': 'singles_day'\n",
    "    }\n",
    "\n",
    "def batch_calculate_offer_overlap(df_trans, df_offers, customer_ids):\n",
    "    \"\"\"Vectorized offer-transaction overlap calculation\"\"\"\n",
    "    # Convert to GPU\n",
    "    gdf_trans = to_cudf(df_trans)\n",
    "    gdf_trans['date'] = cudf.to_datetime(gdf_trans['f370'])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process in batches for memory efficiency\n",
    "    batch_size = 10000\n",
    "    for i in range(0, len(customer_ids), batch_size):\n",
    "        batch_customers = customer_ids[i:i+batch_size]\n",
    "        \n",
    "        # Filter transactions for batch\n",
    "        batch_trans = gdf_trans[gdf_trans['id2'].isin(batch_customers)]\n",
    "        \n",
    "        # Vectorized calculations for each offer\n",
    "        for _, offer in df_offers.iterrows():\n",
    "            offer_start = pd.to_datetime(offer['id12'])\n",
    "            offer_end = pd.to_datetime(offer['id13'])\n",
    "            \n",
    "            # Transactions during offer\n",
    "            mask = (batch_trans['date'] >= offer_start) & (batch_trans['date'] <= offer_end)\n",
    "            during_offer = batch_trans[mask]\n",
    "            \n",
    "            # Group by customer\n",
    "            during_offer['day_only'] = during_offer['date'].dt.floor('D')\n",
    "            offer_stats = during_offer.groupby('id2').agg({\n",
    "                'f367': ['count', 'sum'],\n",
    "                'day_only': 'nunique'\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Calculate baseline (30 days before)\n",
    "            baseline_mask = (batch_trans['date'] >= offer_start - pd.Timedelta(days=30)) & \\\n",
    "                           (batch_trans['date'] < offer_start)\n",
    "            baseline = batch_trans[baseline_mask]\n",
    "            \n",
    "            baseline_stats = baseline.groupby('id2').agg({\n",
    "                'f367': 'count'\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Merge and calculate features\n",
    "            merged = cudf.DataFrame({'id2': batch_customers})\n",
    "            merged = merged.merge(offer_stats, on='id2', how='left')\n",
    "            merged = merged.merge(baseline_stats, on='id2', how='left', suffixes=('', '_baseline'))\n",
    "            \n",
    "            # Calculate derived features\n",
    "            merged['offer_id'] = offer['id3']\n",
    "            merged['trans_count_during_offer'] = merged[('f367', 'count')].fillna(0)\n",
    "            merged['trans_amount_during_offer'] = merged[('f367', 'sum')].fillna(0)\n",
    "            merged['baseline_trans_rate'] = merged['f367_baseline'].fillna(0) / 30\n",
    "            merged['offer_period_lift'] = merged['trans_count_during_offer'] / \\\n",
    "                                        (merged['baseline_trans_rate'] + 1e-6)\n",
    "            \n",
    "            results.append(merged.to_pandas())\n",
    "    \n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "def vectorized_holiday_features(dates):\n",
    "    \"\"\"Vectorized holiday feature calculation using GPU\"\"\"\n",
    "    holidays = define_holidays()\n",
    "    holiday_dates = pd.to_datetime(list(holidays.keys()))\n",
    "    \n",
    "    dates_gpu = cp.asarray(pd.to_datetime(dates).values.astype('datetime64[D]').astype(int))\n",
    "    holidays_gpu = cp.asarray(holiday_dates.values.astype('datetime64[D]').astype(int))\n",
    "    \n",
    "    # Calculate distance to each holiday for all dates at once\n",
    "    distances = cp.abs(dates_gpu[:, None] - holidays_gpu[None, :])\n",
    "    \n",
    "    # Find nearest holiday\n",
    "    min_distances = cp.min(distances, axis=1)\n",
    "    nearest_holiday_idx = cp.argmin(distances, axis=1)\n",
    "    \n",
    "    # Convert back to pandas for feature creation\n",
    "    features_df = pd.DataFrame({\n",
    "        'days_to_nearest_holiday': cp.asnumpy(min_distances),\n",
    "        'nearest_holiday_idx': cp.asnumpy(nearest_holiday_idx),\n",
    "        'is_holiday_week': cp.asnumpy(min_distances <= 7)\n",
    "    })\n",
    "    \n",
    "    # Map holiday types\n",
    "    holiday_types = list(holidays.values())\n",
    "    features_df['holiday_type'] = features_df['nearest_holiday_idx'].map(\n",
    "        lambda x: holiday_types[x] if x < len(holiday_types) else 'none'\n",
    "    )\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "# ==========================================\n",
    "# 3. GPU-OPTIMIZED PROBABILISTIC MATCHING\n",
    "# ==========================================\n",
    "\n",
    "def build_industry_propensity_gpu(df_event, df_trans, df_offer):\n",
    "    \"\"\"GPU-accelerated industry propensity calculation\"\"\"\n",
    "    # Convert to GPU\n",
    "    gdf_event = to_cudf(df_event)\n",
    "    gdf_trans = to_cudf(df_trans)\n",
    "    gdf_offer = to_cudf(df_offer)\n",
    "    \n",
    "    # Get overlap customers\n",
    "    overlap = gdf_event['id2'].unique().to_pandas()\n",
    "    overlap = set(overlap) & set(gdf_trans['id2'].unique().to_pandas())\n",
    "    overlap = list(overlap)[:50000]  # Limit for memory\n",
    "    \n",
    "    # Get customer industries\n",
    "    counts = gdf_trans.groupby(['id2', 'id8']).size().reset_index(name='counts')\n",
    "    cust_industries = counts.sort_values(['id2', 'counts'], ascending=[True, False]).drop_duplicates('id2')[['id2', 'id8']].reset_index()\n",
    "    \n",
    "    # Join events with offers\n",
    "    event_offer = gdf_event[gdf_event['id2'].isin(overlap)].merge(\n",
    "        gdf_offer[['id3', 'id10']], on='id3', how='left'\n",
    "    )\n",
    "    \n",
    "    # Join with customer industries\n",
    "    event_offer = event_offer.merge(cust_industries, on='id2', how='left')\n",
    "    \n",
    "    # Calculate CTR by industry pair\n",
    "    clicks = event_offer.groupby(['id8', 'id10'])['id7'].count().reset_index(name='clicks')\n",
    "    impressions = event_offer.groupby(['id8', 'id10']).size().reset_index(name='impressions')\n",
    "    \n",
    "    industry_stats = clicks.merge(impressions, on=['id8', 'id10'])\n",
    "    industry_stats.columns = ['customer_industry', 'offer_industry', 'clicks', 'impressions']\n",
    "    \n",
    "    # Bayesian smoothing\n",
    "    alpha, beta = 1, 10\n",
    "    industry_stats['smoothed_ctr'] = (industry_stats['clicks'] + alpha) / \\\n",
    "                                    (industry_stats['impressions'] + beta)\n",
    "    \n",
    "    # Convert to lookup dictionary\n",
    "    stats_pd = industry_stats.to_pandas()\n",
    "    industry_propensity = {\n",
    "        (row['customer_industry'], row['offer_industry']): row['smoothed_ctr']\n",
    "        for _, row in stats_pd.iterrows()\n",
    "    }\n",
    "    \n",
    "    default_ctr = (stats_pd['clicks'].sum() + alpha) / \\\n",
    "                  (stats_pd['impressions'].sum() + beta)\n",
    "    \n",
    "    return industry_propensity, default_ctr\n",
    "\n",
    "def create_behavioral_archetypes_gpu(df_event, df_trans, n_archetypes=15):\n",
    "    \"\"\"GPU-accelerated archetype creation\"\"\"\n",
    "    # Get overlap customers\n",
    "    overlap = set(df_event['id2'].unique()) & set(df_trans['id2'].unique())\n",
    "    sample_customers = list(overlap)[:20000]\n",
    "    \n",
    "    # Create feature matrix on GPU\n",
    "    features_list = []\n",
    "    \n",
    "    # Process in batches\n",
    "    batch_size = 5000\n",
    "    for i in range(0, len(sample_customers), batch_size):\n",
    "        batch = sample_customers[i:i+batch_size]\n",
    "        \n",
    "        # Convert to GPU\n",
    "        batch_trans = to_cudf(df_trans[df_trans['id2'].isin(batch)])\n",
    "        batch_event = to_cudf(df_event[df_event['id2'].isin(batch)])\n",
    "        \n",
    "        # Aggregate features\n",
    "        trans_features = batch_trans.groupby('id2').agg({\n",
    "            'f367': ['mean', 'std', 'count']\n",
    "        }).reset_index()\n",
    "        trans_features.columns = ['id2', 'f367_mean', 'f367_std', 'f367_count']\n",
    "        \n",
    "        clicks = batch_event.groupby('id2')['id7'].count().reset_index()\n",
    "        clicks.columns = ['id2', 'clicks']\n",
    "        \n",
    "        impressions = batch_event.groupby('id2')['id3'].count().reset_index()\n",
    "        impressions.columns = ['id2', 'impressions']\n",
    "        \n",
    "        event_features = clicks.merge(impressions, on='id2', how='outer')\n",
    "        \n",
    "        # Merge\n",
    "        features = trans_features.merge(event_features, on='id2', how='outer')\n",
    "        features_list.append(features.to_pandas())\n",
    "    \n",
    "    # Combine all features\n",
    "    all_features = pd.concat(features_list, ignore_index=True)\n",
    "    \n",
    "    # Normalize and cluster on GPU\n",
    "    feature_cols = all_features.columns[1:]\n",
    "    X = all_features[feature_cols].fillna(0).values\n",
    "    X_gpu = cp.asarray(X)\n",
    "    \n",
    "    # Scale\n",
    "    scaler = cuStandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_gpu)\n",
    "    \n",
    "    # Cluster\n",
    "    kmeans = cuKMeans(n_clusters=n_archetypes, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    all_features['archetype'] = cp.asnumpy(labels)\n",
    "    \n",
    "    # Calculate archetype profiles\n",
    "    archetype_profiles = all_features.groupby('archetype')[feature_cols].mean()\n",
    "    \n",
    "    # Calculate archetype CTRs\n",
    "    all_features['ctr'] = all_features['clicks'] / (all_features['impressions'] + 1e-6)\n",
    "    archetype_ctr = all_features.groupby('archetype')['ctr'].mean()\n",
    "    \n",
    "    return archetype_profiles, archetype_ctr, scaler, kmeans\n",
    "\n",
    "def create_features_gpu_batch(df_data, df_event, df_trans, df_offer,\n",
    "                             customer_clusters, cluster_profiles,\n",
    "                             industry_propensity, default_ctr,\n",
    "                             batch_size=50000):\n",
    "    \"\"\"Create features in GPU batches\"\"\"\n",
    "    \n",
    "    n_rows = len(df_data)\n",
    "    all_features = []\n",
    "    \n",
    "    # Pre-compute expensive lookups\n",
    "    print(\"   Pre-computing lookup tables...\")\n",
    "    \n",
    "    # Get all unique customers and their transaction/event data\n",
    "    unique_customers = df_data['id2'].unique()\n",
    "    \n",
    "    # Pre-aggregate customer data\n",
    "    print(\"   Aggregating customer transaction data...\")\n",
    "    cust_trans_agg = df_trans.groupby('id2').agg({\n",
    "        'f367': ['mean', 'std', 'count', 'sum'],\n",
    "        'id8': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0]\n",
    "    }).reset_index()\n",
    "    cust_trans_agg.columns = ['id2', 'avg_trans', 'std_trans', 'count_trans', 'sum_trans', 'primary_industry']\n",
    "    \n",
    "    print(\"   Processing in batches...\")\n",
    "    for start_idx in range(0, n_rows, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_rows)\n",
    "        print(f\"   Batch {start_idx//batch_size + 1}/{(n_rows//batch_size) + 1}\")\n",
    "        \n",
    "        batch_data = df_data.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        # Merge pre-aggregated data\n",
    "        batch_data = batch_data.merge(cust_trans_agg, on='id2', how='left')\n",
    "        \n",
    "        # Create features\n",
    "        batch_features = pd.DataFrame(index=batch_data.index)\n",
    "        \n",
    "        # 1. Temporal features (vectorized)\n",
    "        holiday_features = vectorized_holiday_features(batch_data['id5'])\n",
    "        for col in holiday_features.columns:\n",
    "            batch_features[f'temporal_{col}'] = holiday_features[col]\n",
    "        \n",
    "        # 2. Behavioral features\n",
    "        # Check if customer is in overlap\n",
    "        batch_features['behavioral_cluster'] = batch_data['id2'].map(\n",
    "            customer_clusters\n",
    "        ).fillna(-1)\n",
    "        \n",
    "        # Get cluster averages\n",
    "        cluster_map = cluster_profiles['ctr'].to_dict()\n",
    "        batch_features['behavioral_cluster_ctr'] = batch_features['behavioral_cluster'].map(\n",
    "            cluster_map\n",
    "        ).fillna(0)\n",
    "        \n",
    "        # 3. Probabilistic features\n",
    "        # Get offer industries\n",
    "        offer_industries = df_offer.set_index('id3')['id10'].to_dict()\n",
    "        batch_data['offer_industry'] = batch_data['id3'].map(offer_industries)\n",
    "        \n",
    "        # Build tuple keys\n",
    "        batch_keys = list(zip(batch_data['primary_industry'], batch_data['offer_industry']))\n",
    "        batch_keys_series = pd.Series(batch_keys)\n",
    "        \n",
    "        # Vectorized lookup with .map()\n",
    "        batch_features['prob_industry_propensity'] = batch_keys_series.map(\n",
    "            lambda k: industry_propensity.get(k, default_ctr)\n",
    "        )\n",
    "        \n",
    "        # Add basic stats as features\n",
    "        batch_features['customer_avg_trans'] = batch_data['avg_trans'].fillna(0)\n",
    "        batch_features['customer_trans_count'] = batch_data['count_trans'].fillna(0)\n",
    "        batch_features['customer_trans_cv'] = (batch_data['std_trans'] / \n",
    "                                               (batch_data['avg_trans'] + 1e-6)).fillna(0)\n",
    "        \n",
    "        all_features.append(batch_features)\n",
    "    \n",
    "    return pd.concat(all_features, ignore_index=True)\n",
    "\n",
    "# ==========================================\n",
    "# MAIN GPU PIPELINE FUNCTION\n",
    "# ==========================================\n",
    "\n",
    "def add_gpu_features(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame,\n",
    "                     df_event: pd.DataFrame, df_trans: pd.DataFrame, df_offer: pd.DataFrame,\n",
    "                     sample_size: int = None, batch_size: int = 50000, \n",
    "                     n_clusters: int = 25) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Add GPU-accelerated behavioral, temporal, and probabilistic features to train, validation, and test DataFrames.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_df: Training DataFrame\n",
    "    - val_df: Validation DataFrame  \n",
    "    - test_df: Test DataFrame\n",
    "    - df_event: Event data DataFrame\n",
    "    - df_trans: Transaction data DataFrame\n",
    "    - df_offer: Offer data DataFrame\n",
    "    - sample_size: Optional sample size for faster processing\n",
    "    - batch_size: Batch size for GPU processing\n",
    "    - n_clusters: Number of behavioral clusters\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of (train_df, val_df, test_df) with GPU-accelerated features added\n",
    "    \"\"\"\n",
    "    print(\"=== Starting GPU-Accelerated Feature Pipeline ===\")\n",
    "    \n",
    "    # Create copies to avoid modifying original DataFrames\n",
    "    train_df = train_df.copy()\n",
    "    val_df = val_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    # Sample if requested\n",
    "    if sample_size:\n",
    "        train_df = train_df.sample(n=sample_size, random_state=42)\n",
    "        val_df = val_df.sample(n=min(sample_size//4, len(val_df)), random_state=42)\n",
    "        test_df = test_df.sample(n=min(sample_size//4, len(test_df)), random_state=42)\n",
    "    \n",
    "    # Ensure data types\n",
    "    for df in [train_df, val_df, test_df, df_event, df_trans, df_offer]:\n",
    "        if 'id2' in df.columns:\n",
    "            df['id2'] = df['id2'].astype(str)\n",
    "        if 'id3' in df.columns:\n",
    "            df['id3'] = df['id3'].astype(str)\n",
    "    \n",
    "    # Step 1: Build models from overlap data\n",
    "    print(\"\\n1. Finding overlapping customers...\")\n",
    "    overlap_customers = identify_overlap_customers(df_event, df_trans)\n",
    "    \n",
    "    print(\"\\n2. Creating behavioral profiles on GPU...\")\n",
    "    profiles = create_customer_profiles_gpu(\n",
    "        df_event, df_trans, \n",
    "        overlap_customers[:min(50000, len(overlap_customers))]\n",
    "    )\n",
    "    \n",
    "    print(\"\\n3. Creating behavioral clusters on GPU...\")\n",
    "    profiles_clustered, cluster_profiles, customer_clusters, scaler, kmeans = create_behavioral_clusters_gpu(\n",
    "        profiles, n_clusters=n_clusters\n",
    "    )\n",
    "    \n",
    "    print(\"\\n4. Building industry propensity on GPU...\")\n",
    "    industry_propensity, default_ctr = build_industry_propensity_gpu(df_event, df_trans, df_offer)\n",
    "    \n",
    "    print(\"\\n5. Creating behavioral archetypes on GPU...\")\n",
    "    archetype_profiles, archetype_ctr, arch_scaler, arch_kmeans = create_behavioral_archetypes_gpu(\n",
    "        df_event, df_trans\n",
    "    )\n",
    "    \n",
    "    # Step 2: Create features for train/val/test\n",
    "    print(\"\\n6. Creating features for training data...\")\n",
    "    train_features = create_features_gpu_batch(\n",
    "        train_df, df_event, df_trans, df_offer,\n",
    "        customer_clusters, cluster_profiles,\n",
    "        industry_propensity, default_ctr,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    print(\"\\n7. Creating features for validation data...\")\n",
    "    val_features = create_features_gpu_batch(\n",
    "        val_df, df_event, df_trans, df_offer,\n",
    "        customer_clusters, cluster_profiles,\n",
    "        industry_propensity, default_ctr,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    print(\"\\n8. Creating features for test data...\")\n",
    "    test_features = create_features_gpu_batch(\n",
    "        test_df, df_event, df_trans, df_offer,\n",
    "        customer_clusters, cluster_profiles,\n",
    "        industry_propensity, default_ctr,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Merge features\n",
    "    train_df_enhanced = pd.concat([train_df.reset_index(drop=True), train_features], axis=1)\n",
    "    val_df_enhanced = pd.concat([val_df.reset_index(drop=True), val_features], axis=1)\n",
    "    test_df_enhanced = pd.concat([test_df.reset_index(drop=True), test_features], axis=1)\n",
    "    \n",
    "    # List of new features\n",
    "    new_features = [\n",
    "        'temporal_days_to_nearest_holiday', 'temporal_nearest_holiday_idx', 'temporal_is_holiday_week',\n",
    "        'temporal_holiday_type', 'behavioral_cluster', 'behavioral_cluster_ctr',\n",
    "        'prob_industry_propensity', 'customer_avg_trans', 'customer_trans_count', 'customer_trans_cv'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== GPU Feature Engineering Complete ===\")\n",
    "    print(f\"Training shape: {train_df_enhanced.shape}\")\n",
    "    print(f\"Validation shape: {val_df_enhanced.shape}\")\n",
    "    print(f\"Test shape: {test_df_enhanced.shape}\")\n",
    "    print(\"Feature engineering completed. Added GPU features:\")\n",
    "    print(new_features)\n",
    "    \n",
    "    return train_df_enhanced, val_df_enhanced, test_df_enhanced\n",
    "\n",
    "# ==========================================\n",
    "# QUICK EXAMPLE FUNCTION\n",
    "# ==========================================\n",
    "\n",
    "def quick_gpu_example(train_df, val_df, test_df, df_event, df_trans, df_offer, sample_size=10000):\n",
    "    \"\"\"Quick example with GPU acceleration\"\"\"\n",
    "    try:\n",
    "        print(\"Running GPU-accelerated feature engineering...\")\n",
    "        \n",
    "        # Run pipeline\n",
    "        train_gpu, val_gpu, test_gpu = add_gpu_features(\n",
    "            train_df, val_df, test_df, df_event, df_trans, df_offer,\n",
    "            sample_size=sample_size,\n",
    "            batch_size=50000\n",
    "        )\n",
    "        \n",
    "        print(\"\\nSample of created features:\")\n",
    "        gpu_features = [col for col in train_gpu.columns \n",
    "                       if col.startswith(('temporal_', 'behavioral_', 'prob_', 'customer_'))]\n",
    "        print(f\"Created {len(gpu_features)} new features\")\n",
    "        print(gpu_features[:10])\n",
    "        \n",
    "        return train_gpu, val_gpu, test_gpu\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc2941c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:46:01.348957Z",
     "iopub.status.busy": "2025-07-20T17:46:01.348422Z",
     "iopub.status.idle": "2025-07-20T17:48:40.321782Z",
     "shell.execute_reply": "2025-07-20T17:48:40.321229Z",
     "shell.execute_reply.started": "2025-07-20T17:46:01.348938Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = quick_gpu_example(train_df, val_df, test_df, df_event, df_trans, df_offer, sample_size=False)\n",
    "del df_event, df_trans, df_offer\n",
    "gc.collect()\n",
    "\n",
    "# Clear GPU memory if available\n",
    "try:\n",
    "    import cupy as cp\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"Memory freed after GPU processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b80500",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:48:40.322879Z",
     "iopub.status.busy": "2025-07-20T17:48:40.322687Z",
     "iopub.status.idle": "2025-07-20T17:48:40.328052Z",
     "shell.execute_reply": "2025-07-20T17:48:40.327203Z",
     "shell.execute_reply.started": "2025-07-20T17:48:40.322864Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def add_bayesian_features(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "#     \"\"\"\n",
    "#     Add Bayesian-smoothed features to train, validation, and test DataFrames based on customer (id2) and offer (id3) metrics.\n",
    "#     Avoids using the target variable 'y' to prevent target leakage.\n",
    "\n",
    "#     Parameters:\n",
    "#     - train_df: Training DataFrame with 'id2', 'id3', 'f132', 'best_day_ctr', 'overall_ctr'\n",
    "#     - val_df: Validation DataFrame with 'id2', 'id3', 'f132', 'best_day_ctr', 'overall_ctr'\n",
    "#     - test_df: Test DataFrame with 'id2', 'id3', 'f132', 'best_day_ctr', 'overall_ctr'\n",
    "\n",
    "#     Returns:\n",
    "#     - Tuple of (train_df, val_df, test_df) with Bayesian features added\n",
    "#     \"\"\"\n",
    "#     # Create copies to avoid modifying original DataFrames\n",
    "#     train_df = train_df.copy()\n",
    "#     val_df = val_df.copy()\n",
    "#     test_df = test_df.copy()\n",
    "\n",
    "#     # Check for required columns\n",
    "#     required_cols = ['id2', 'id3', 'f132', 'best_day_ctr', 'overall_ctr']\n",
    "#     # Training and validation require 'y', but test may not\n",
    "#     for df, name in [(train_df, 'train_df'), (val_df, 'val_df')]:\n",
    "#         missing_cols = [col for col in required_cols + ['y'] if col not in df.columns]\n",
    "#         if missing_cols:\n",
    "#             raise KeyError(f\"Missing columns in {name}: {missing_cols}\")\n",
    "#     # Test DataFrame may not have 'y'\n",
    "#     missing_cols_test = [col for col in required_cols if col not in test_df.columns]\n",
    "#     if missing_cols_test:\n",
    "#         raise KeyError(f\"Missing columns in test_df: {missing_cols_test}\")\n",
    "\n",
    "#     # Bayesian smoothing function\n",
    "#     def bayesian_smoothing(group, value_col, prior_mean, prior_weight=10):\n",
    "#         \"\"\"\n",
    "#         Apply Bayesian smoothing to a value column.\n",
    "#         - prior_mean: Global mean of the value column\n",
    "#         - prior_weight: Weight of the prior (controls smoothing strength)\n",
    "#         \"\"\"\n",
    "#         counts = group.count()\n",
    "#         means = group.mean()\n",
    "#         smoothed = (means * counts + prior_mean * prior_weight) / (counts + prior_weight)\n",
    "#         return smoothed\n",
    "\n",
    "#     # 1. Bayesian-Smoothed Features for Customer (id2)\n",
    "#     print(\"Adding Bayesian-smoothed features for customers (id2)...\")\n",
    "#     # Compute global means from training data only to avoid leakage\n",
    "#     global_f132_mean = train_df['f132'].mean()\n",
    "#     global_best_day_ctr_mean = train_df['best_day_ctr'].mean()\n",
    "#     global_overall_ctr_mean = train_df['overall_ctr'].mean()\n",
    "\n",
    "#     for df, name in [(train_df, 'train_df'), (val_df, 'val_df'), (test_df, 'test_df')]:\n",
    "#         # Bayesian-smoothed f132 per customer\n",
    "#         smoothed_f132 = df.groupby('id2')['f132'].apply(lambda x: bayesian_smoothing(x, 'f132', global_f132_mean))\n",
    "#         df['bayes_f132_per_id2'] = df['id2'].map(smoothed_f132)\n",
    "\n",
    "#         # Bayesian-smoothed best_day_ctr per customer\n",
    "#         smoothed_best_day_ctr = df.groupby('id2')['best_day_ctr'].apply(lambda x: bayesian_smoothing(x, 'best_day_ctr', global_best_day_ctr_mean))\n",
    "#         df['bayes_best_day_ctr_per_id2'] = df['id2'].map(smoothed_best_day_ctr)\n",
    "\n",
    "#         # Bayesian-smoothed overall_ctr per customer\n",
    "#         smoothed_overall_ctr = df.groupby('id2')['overall_ctr'].apply(lambda x: bayesian_smoothing(x, 'overall_ctr', global_overall_ctr_mean))\n",
    "#         df['bayes_overall_ctr_per_id2'] = df['id2'].map(smoothed_overall_ctr)\n",
    "\n",
    "#     # 2. Bayesian-Smoothed Features for Offer (id3)\n",
    "#     print(\"Adding Bayesian-smoothed features for offers (id3)...\")\n",
    "#     for df, name in [(train_df, 'train_df'), (val_df, 'val_df'), (test_df, 'test_df')]:\n",
    "#         # Bayesian-smoothed f132 per offer\n",
    "#         smoothed_f132_offer = df.groupby('id3')['f132'].apply(lambda x: bayesian_smoothing(x, 'f132', global_f132_mean))\n",
    "#         df['bayes_f132_per_id3'] = df['id3'].map(smoothed_f132_offer)\n",
    "\n",
    "#         # Bayesian-smoothed best_day_ctr per offer\n",
    "#         smoothed_best_day_ctr_offer = df.groupby('id3')['best_day_ctr'].apply(lambda x: bayesian_smoothing(x, 'best_day_ctr', global_best_day_ctr_mean))\n",
    "#         df['bayes_best_day_ctr_per_id3'] = df['id3'].map(smoothed_best_day_ctr_offer)\n",
    "\n",
    "#         # Bayesian-smoothed overall_ctr per offer\n",
    "#         smoothed_overall_ctr_offer = df.groupby('id3')['overall_ctr'].apply(lambda x: bayesian_smoothing(x, 'overall_ctr', global_overall_ctr_mean))\n",
    "#         df['bayes_overall_ctr_per_id3'] = df['id3'].map(smoothed_overall_ctr_offer)\n",
    "\n",
    "#     # List of new features\n",
    "#     new_features = [\n",
    "#         'bayes_f132_per_id2', 'bayes_best_day_ctr_per_id2', 'bayes_overall_ctr_per_id2',\n",
    "#         'bayes_f132_per_id3', 'bayes_best_day_ctr_per_id3', 'bayes_overall_ctr_per_id3'\n",
    "#     ]\n",
    "#     print(\"Bayesian feature engineering completed. Added features:\")\n",
    "#     print(new_features)\n",
    "\n",
    "#     return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403c363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:48:40.329127Z",
     "iopub.status.busy": "2025-07-20T17:48:40.328829Z",
     "iopub.status.idle": "2025-07-20T17:48:40.454157Z",
     "shell.execute_reply": "2025-07-20T17:48:40.453587Z",
     "shell.execute_reply.started": "2025-07-20T17:48:40.329110Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ec7d0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:48:40.455092Z",
     "iopub.status.busy": "2025-07-20T17:48:40.454857Z",
     "iopub.status.idle": "2025-07-20T17:50:36.706212Z",
     "shell.execute_reply": "2025-07-20T17:50:36.705485Z",
     "shell.execute_reply.started": "2025-07-20T17:48:40.455066Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, val_df, test_df =  add_new_features(train_df, val_df, test_df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9773e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:50:36.707331Z",
     "iopub.status.busy": "2025-07-20T17:50:36.707009Z",
     "iopub.status.idle": "2025-07-20T17:50:36.843864Z",
     "shell.execute_reply": "2025-07-20T17:50:36.843216Z",
     "shell.execute_reply.started": "2025-07-20T17:50:36.707304Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebdb3dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:50:36.845464Z",
     "iopub.status.busy": "2025-07-20T17:50:36.844758Z",
     "iopub.status.idle": "2025-07-20T17:50:41.783537Z",
     "shell.execute_reply": "2025-07-20T17:50:41.782329Z",
     "shell.execute_reply.started": "2025-07-20T17:50:36.845438Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_bayesian_features(train_df, val_df, test_df):\n",
    "    \"\"\"Add Bayesian smoothed CTR features with hierarchical priors\"\"\"\n",
    "    \n",
    "    # Global priors from training data\n",
    "    global_ctr = train_df['y'].mean()\n",
    "    alpha, beta = 1, 1/global_ctr - 1  # Beta prior parameters\n",
    "    \n",
    "    # 1. Customer-level Bayesian CTR\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        # Get customer click/impression counts from features\n",
    "        customer_stats = df.groupby('id2').agg({\n",
    "            'f147': 'sum',  # clicks in last 30 days\n",
    "            'f146': 'sum'   # impressions in last 14 days\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Bayesian smoothing\n",
    "        customer_stats['bayesian_customer_ctr'] = (\n",
    "            (customer_stats['f147'] + alpha) / \n",
    "            (customer_stats['f146'] + alpha + beta)\n",
    "        )\n",
    "        \n",
    "        df = df.merge(customer_stats[['id2', 'bayesian_customer_ctr']], on='id2', how='left')\n",
    "    \n",
    "    # 2. Offer-level Bayesian CTR\n",
    "    offer_stats = train_df.groupby('id3')['y'].agg(['sum', 'count']).reset_index()\n",
    "    offer_stats['bayesian_offer_ctr'] = (\n",
    "        (offer_stats['sum'] + alpha) / \n",
    "        (offer_stats['count'] + alpha + beta)\n",
    "    )\n",
    "    \n",
    "    # 3. Customer-Category Bayesian CTR\n",
    "    # Using f123-f145 (category-specific clicks)\n",
    "    category_features = []\n",
    "    for i in range(123, 146):\n",
    "        if f'f{i}' in train_df.columns:\n",
    "            category_features.append(f'f{i}')\n",
    "    \n",
    "    # 4. Time-decay weighted Bayesian CTR\n",
    "    df['time_weighted_bayesian_ctr'] = (\n",
    "        (df['f29'] + alpha) /  # decaying clicks\n",
    "        (df['f28'] + alpha + beta)  # decaying impressions\n",
    "    )\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "train_df, val_df, test_df = add_bayesian_features(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1d5c49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:50:41.784908Z",
     "iopub.status.busy": "2025-07-20T17:50:41.784630Z",
     "iopub.status.idle": "2025-07-20T17:50:46.258297Z",
     "shell.execute_reply": "2025-07-20T17:50:46.257472Z",
     "shell.execute_reply.started": "2025-07-20T17:50:41.784876Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import cupy as cp\n",
    "from cuml.cluster import KMeans as cuKMeans\n",
    "from cuml.preprocessing import StandardScaler as cuStandardScaler\n",
    "\n",
    "def add_interest_segmentation_features_gpu(train_df, val_df, test_df):\n",
    "    \"\"\"Memory-efficient GPU feature engineering for interest segmentation\"\"\"\n",
    "\n",
    "    interest_cols = [f'f{i}' for i in range(1, 13)]\n",
    "\n",
    "    def add_features(df):\n",
    "        df_gpu = cudf.DataFrame(df[interest_cols])  # Only f1-f12 to GPU\n",
    "        arr = df_gpu.to_cupy()\n",
    "\n",
    "        # 1. Entropy (vectorized)\n",
    "        row_sums = cp.sum(arr, axis=1).reshape(-1, 1)\n",
    "        p = cp.where(row_sums != 0, arr / row_sums, 0.0)\n",
    "        entropy = -cp.sum(cp.where(p != 0, p * cp.log(p + 1e-10), 0.0), axis=1)\n",
    "\n",
    "        # 2. Dominant interest\n",
    "        dominant_interest = cp.argmax(arr, axis=1)\n",
    "        dominant_interest_score = cp.max(arr, axis=1)\n",
    "\n",
    "        # 3. Gini coefficient (vectorized)\n",
    "        x_sorted = cp.sort(arr, axis=1)\n",
    "        n = arr.shape[1]\n",
    "        index = cp.arange(1, n + 1).reshape(1, -1)\n",
    "        gini = (2 * cp.sum(index * x_sorted, axis=1)) / (n * cp.sum(arr, axis=1) + 1e-10) - (n + 1) / n\n",
    "\n",
    "        # 4. Multi-interest indicator (> mean of the row)\n",
    "        row_means = cp.mean(arr, axis=1).reshape(-1, 1)\n",
    "        multi_interest_count = cp.sum(arr > row_means, axis=1)\n",
    "\n",
    "        # Save features back to CPU dataframe\n",
    "        df['interest_entropy'] = cp.asnumpy(entropy)\n",
    "        df['dominant_interest'] = cp.asnumpy(dominant_interest)\n",
    "        df['dominant_interest_score'] = cp.asnumpy(dominant_interest_score)\n",
    "        df['interest_concentration'] = cp.asnumpy(gini)\n",
    "        df['multi_interest_count'] = cp.asnumpy(multi_interest_count)\n",
    "\n",
    "        return df\n",
    "\n",
    "    # Add features to each dataframe\n",
    "    train_df = add_features(train_df)\n",
    "    val_df = add_features(val_df)\n",
    "    test_df = add_features(test_df)\n",
    "\n",
    "    # GPU clustering (KMeans)\n",
    "    scaler = cuStandardScaler()\n",
    "    train_gpu = cudf.DataFrame(train_df[interest_cols])\n",
    "    train_scaled = scaler.fit_transform(train_gpu)\n",
    "\n",
    "    kmeans = cuKMeans(n_clusters=20, random_state=42)\n",
    "    train_df['interest_cluster'] = cp.asnumpy(kmeans.fit_predict(train_scaled))\n",
    "\n",
    "    for df in [val_df, test_df]:\n",
    "        df_gpu = cudf.DataFrame(df[interest_cols])\n",
    "        df_scaled = scaler.transform(df_gpu)\n",
    "        df['interest_cluster'] = cp.asnumpy(kmeans.predict(df_scaled))\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = add_interest_segmentation_features_gpu(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a616e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:50:46.259806Z",
     "iopub.status.busy": "2025-07-20T17:50:46.259543Z",
     "iopub.status.idle": "2025-07-20T17:50:46.667275Z",
     "shell.execute_reply": "2025-07-20T17:50:46.666642Z",
     "shell.execute_reply.started": "2025-07-20T17:50:46.259780Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_spend_velocity_features(train_df, val_df, test_df):\n",
    "    \"\"\"Create spend-based velocity and pattern features\"\"\"\n",
    "    \n",
    "    # Spend columns: f39 (Lifestyle), f40 (Electronics), f41 (Restaurant)\n",
    "    spend_cols = ['f39', 'f40', 'f41']\n",
    "    \n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        # 1. Total spend across categories\n",
    "        df['total_spend_3m'] = df[spend_cols].sum(axis=1)\n",
    "        \n",
    "        # 2. Spend diversity (which categories)\n",
    "        df['spend_categories_active'] = (df[spend_cols] > 0).sum(axis=1)\n",
    "        \n",
    "        # 3. Spend concentration\n",
    "        df['spend_concentration'] = df[spend_cols].max(axis=1) / (df['total_spend_3m'] + 1e-10)\n",
    "        \n",
    "        # 4. Category spend ratios\n",
    "        for col in spend_cols:\n",
    "            df[f'{col}_ratio'] = df[col] / (df['total_spend_3m'] + 1e-10)\n",
    "        \n",
    "        # 5. Spend vs. click alignment\n",
    "        df['restaurant_spend_click_ratio'] = df['f41'] / (df['f125'] + 1)  # f125: restaurant clicks\n",
    "        df['electronics_spend_impression_ratio'] = df['f40'] / (df['f366'] + 1)\n",
    "        \n",
    "        # 6. Big spender indicator\n",
    "        df['is_big_spender'] = (df['total_spend_3m'] > df['total_spend_3m'].quantile(0.9)).astype(int)\n",
    "        \n",
    "        # 7. Spend velocity (if we had historical data)\n",
    "        # This would compare 3-month spend to previous periods\n",
    "        \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = add_spend_velocity_features(train_df, val_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d31c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:50:46.668255Z",
     "iopub.status.busy": "2025-07-20T17:50:46.667999Z",
     "iopub.status.idle": "2025-07-20T17:50:47.026578Z",
     "shell.execute_reply": "2025-07-20T17:50:47.025960Z",
     "shell.execute_reply.started": "2025-07-20T17:50:46.668233Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_page_engagement_features(train_df, val_df, test_df):\n",
    "    \"\"\"Create features from page visit patterns, using only available columns\"\"\"\n",
    "\n",
    "    # Define the expected columns\n",
    "    page_visit_cols = [f'f{i}' for i in range(14, 22)]\n",
    "    time_30d_cols = [f'f{i}' for i in range(59, 68)]\n",
    "    time_180d_cols = [f'f{i}' for i in range(68, 77)]\n",
    "\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        # 1. Page visit count (only use available columns)\n",
    "        existing_page_cols = [col for col in page_visit_cols if col in df.columns]\n",
    "        if existing_page_cols:\n",
    "            df['pages_visited_count'] = df[existing_page_cols].sum(axis=1)\n",
    "        else:\n",
    "            df['pages_visited_count'] = 0\n",
    "\n",
    "        # 2. Engagement intensity\n",
    "        df['total_time_30d'] = df['f59'] if 'f59' in df.columns else 0\n",
    "        df['total_time_180d'] = df['f68'] if 'f68' in df.columns else 1e-10  # Avoid division by zero\n",
    "\n",
    "        # 3. Recency ratio\n",
    "        df['engagement_recency_ratio'] = df['total_time_30d'] / (df['total_time_180d'] + 1e-10)\n",
    "\n",
    "        # 4. Page-specific engagement rates\n",
    "        df['redemption_engagement'] = (df['f14'] * df['f61']) if 'f14' in df.columns and 'f61' in df.columns else 0\n",
    "        df['travel_engagement'] = (df['f18'] * df['f65']) if 'f18' in df.columns and 'f65' in df.columns else 0\n",
    "\n",
    "        # 5. Engagement consistency (CV of page times)\n",
    "        existing_time_30d_cols = [col for col in time_30d_cols if col in df.columns]\n",
    "        if existing_time_30d_cols:\n",
    "            df['page_time_cv'] = df[existing_time_30d_cols].std(axis=1) / (df[existing_time_30d_cols].mean(axis=1) + 1e-10)\n",
    "        else:\n",
    "            df['page_time_cv'] = 0\n",
    "\n",
    "        # 6. Focus score\n",
    "        if existing_time_30d_cols:\n",
    "            page_times = df[existing_time_30d_cols].values\n",
    "            df['page_focus_score'] = np.max(page_times, axis=1) / (np.sum(page_times, axis=1) + 1e-10)\n",
    "        else:\n",
    "            df['page_focus_score'] = 0\n",
    "\n",
    "        # 7. Engagement trend\n",
    "        df['engagement_trend'] = (df['f77'] - 0.5) * 2 if 'f77' in df.columns else 0\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = add_page_engagement_features(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f5454f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:50:47.031480Z",
     "iopub.status.busy": "2025-07-20T17:50:47.030957Z",
     "iopub.status.idle": "2025-07-20T17:50:47.108792Z",
     "shell.execute_reply": "2025-07-20T17:50:47.108183Z",
     "shell.execute_reply.started": "2025-07-20T17:50:47.031459Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_advanced_temporal_features(train_df, val_df, test_df):\n",
    "    \"\"\"Create sophisticated temporal features\"\"\"\n",
    "    \n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        # 1. Click velocity changes\n",
    "        df['click_acceleration'] = (df['f201'] - df['f147']) / 14  # 14-day vs 30-day\n",
    "        \n",
    "        # 2. Impression fatigue\n",
    "        df['impression_fatigue_score'] = df['f203'] / (df['f313'] + 1e-10)  # 30d vs 14d CTR\n",
    "        \n",
    "        # 3. Temporal consistency\n",
    "        # Using various time window features\n",
    "        click_windows = {\n",
    "            '7d': 'f209',   # 7-day clicks\n",
    "            '14d': 'f201',  # 14-day clicks\n",
    "            '30d': 'f147',  # 30-day clicks\n",
    "            '60d': 'f123',  # 60-day clicks (dining category as proxy)\n",
    "        }\n",
    "        \n",
    "        if all(col in df.columns for col in click_windows.values()):\n",
    "            click_values = [df[col] for col in click_windows.values()]\n",
    "            df['click_temporal_std'] = np.std(click_values, axis=0)\n",
    "            df['click_temporal_cv'] = df['click_temporal_std'] / (np.mean(click_values, axis=0) + 1e-10)\n",
    "        \n",
    "        # 4. Day-of-week patterns (from existing datetime features)\n",
    "        if 'day_of_week' in df.columns:\n",
    "            # Assuming we can aggregate historical data\n",
    "            df['is_weekend_person'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "        \n",
    "        # 5. Response time features\n",
    "        # Time between impression and click (would need event-level data)\n",
    "        \n",
    "    return train_df, val_df, test_df\n",
    "train_df, val_df, test_df = add_advanced_temporal_features(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3664bcdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:50:47.109601Z",
     "iopub.status.busy": "2025-07-20T17:50:47.109401Z",
     "iopub.status.idle": "2025-07-20T17:50:47.271035Z",
     "shell.execute_reply": "2025-07-20T17:50:47.270501Z",
     "shell.execute_reply.started": "2025-07-20T17:50:47.109585Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4c1c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:50:47.272036Z",
     "iopub.status.busy": "2025-07-20T17:50:47.271783Z",
     "iopub.status.idle": "2025-07-20T17:52:32.597842Z",
     "shell.execute_reply": "2025-07-20T17:52:32.597274Z",
     "shell.execute_reply.started": "2025-07-20T17:50:47.272011Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_statistical_features(train_df, val_df, test_df):\n",
    "    \"\"\"Add statistical distribution features\"\"\"\n",
    "    \n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        # 1. Z-scores for key metrics\n",
    "        for col in ['f132', 'f366', 'overall_ctr']:\n",
    "            mean = train_df[col].mean()\n",
    "            std = train_df[col].std()\n",
    "            df[f'{col}_zscore'] = (df[col] - mean) / (std + 1e-10)\n",
    "        \n",
    "        # 2. Percentile ranks\n",
    "        for col in ['f132', 'best_day_ctr', 'f366']:\n",
    "            df[f'{col}_percentile'] = df[col].rank(pct=True)\n",
    "        \n",
    "        # 3. Outlier indicators\n",
    "        df['is_ctr_outlier'] = (np.abs(df['overall_ctr_zscore']) > 3).astype(int)\n",
    "        \n",
    "        # 4. Relative performance metrics\n",
    "        df['ctr_vs_median'] = df['overall_ctr'] / df['overall_ctr'].median()\n",
    "        \n",
    "        # 5. Distribution moments\n",
    "        numeric_cols = ['f132', 'f366', 'best_day_ctr', 'worst_day_ctr']\n",
    "        df['metric_skewness'] = df[numeric_cols].apply(lambda x: x.skew(), axis=1)\n",
    "        df['metric_kurtosis'] = df[numeric_cols].apply(lambda x: x.kurtosis(), axis=1)\n",
    "        \n",
    "    return train_df, val_df, test_df\n",
    "train_df, val_df, test_df = add_statistical_features(train_df, val_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7807616f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:52:32.599220Z",
     "iopub.status.busy": "2025-07-20T17:52:32.598638Z",
     "iopub.status.idle": "2025-07-20T17:52:32.664756Z",
     "shell.execute_reply": "2025-07-20T17:52:32.663985Z",
     "shell.execute_reply.started": "2025-07-20T17:52:32.599194Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_cross_interactions(train_df, val_df, test_df):\n",
    "    \"\"\"Create meaningful cross-feature interactions\"\"\"\n",
    "    \n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        # 1. Interest × Spend interactions\n",
    "        df['travel_interest_spend_alignment'] = df['f8'] * df['f65']  # Travel interest × travel page time\n",
    "        \n",
    "        # 2. CTR × Category interactions\n",
    "        df['high_ctr_dining'] = df['overall_ctr'] * df['f125']  # CTR × dining clicks\n",
    "        \n",
    "        # 3. Temporal × Behavioral interactions\n",
    "        df['weekend_high_value'] = df.get('is_weekend_person', 0) * df.get('is_big_spender', 0)\n",
    "        \n",
    "        # 4. Engagement × Performance\n",
    "        df['engaged_high_ctr'] = df['f59'] * df['overall_ctr']  # Time spent × CTR\n",
    "        \n",
    "        # 5. Offer × Customer type interactions\n",
    "        df['elite_premium_offer'] = df['f46'] * df.get('offer_value_score', 0)\n",
    "        \n",
    "        # 6. Polynomial features for top predictors\n",
    "        top_features = ['f132', 'f366', 'best_day_ctr']\n",
    "        for feat in top_features:\n",
    "            df[f'{feat}_squared'] = df[feat] ** 2\n",
    "            df[f'{feat}_log'] = np.log1p(df[feat])\n",
    "        \n",
    "        # 7. Ratio interactions\n",
    "        df['impression_click_efficiency'] = df['f147'] / (df['f146'] + 1) * df['overall_ctr']\n",
    "        \n",
    "    return train_df, val_df, test_df\n",
    "train_df, val_df, test_df = add_cross_interactions(train_df, val_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9a4b04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:52:32.665818Z",
     "iopub.status.busy": "2025-07-20T17:52:32.665582Z",
     "iopub.status.idle": "2025-07-20T17:52:33.075064Z",
     "shell.execute_reply": "2025-07-20T17:52:33.074310Z",
     "shell.execute_reply.started": "2025-07-20T17:52:32.665795Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_merchant_affinity_features(train_df, val_df, test_df):\n",
    "    \"\"\"Create merchant and offer-type affinity features\"\"\"\n",
    "    \n",
    "    # Offer type CTR features (f104-f117)\n",
    "    offer_type_cols = {\n",
    "        'rewards': 'f104',\n",
    "        'benefit': 'f105',\n",
    "        'acquisition': 'f106',\n",
    "        'upgrade': 'f107',\n",
    "        'retail': 'f108',\n",
    "        'servicing': 'f111',\n",
    "        'insurance': 'f112'\n",
    "    }\n",
    "    \n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        # 1. Best performing offer type\n",
    "        offer_ctrs = df[[col for col in offer_type_cols.values() if col in df.columns]]\n",
    "        df['best_offer_type_ctr'] = offer_ctrs.max(axis=1)\n",
    "        df['preferred_offer_type'] = offer_ctrs.idxmax(axis=1)\n",
    "        \n",
    "        # 2. Offer type diversity\n",
    "        df['offer_type_breadth'] = (offer_ctrs > 0).sum(axis=1)\n",
    "        \n",
    "        # 3. Merchant vs non-merchant preference\n",
    "        df['merchant_preference_ratio'] = df['f203'] / (df['f110'] + 1e-10)\n",
    "        \n",
    "        # 4. Category-specific momentum\n",
    "        df['dining_momentum'] = df['f123'] / (df['f124'] + 1)  # Recent vs historical\n",
    "        \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = add_merchant_affinity_features(train_df, val_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add0a86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:52:33.076541Z",
     "iopub.status.busy": "2025-07-20T17:52:33.075948Z",
     "iopub.status.idle": "2025-07-20T17:52:33.366747Z",
     "shell.execute_reply": "2025-07-20T17:52:33.366195Z",
     "shell.execute_reply.started": "2025-07-20T17:52:33.076518Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_meta_features(train_df, val_df, test_df):\n",
    "    \"\"\"Create meta-features from other features, handling missing columns safely\"\"\"\n",
    "    \n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        # 1. Feature completeness (only use features that exist)\n",
    "        important_features = ['f132', 'f366', 'best_day_ctr', 'f209', 'f313']\n",
    "        available_important = [col for col in important_features if col in df.columns]\n",
    "        \n",
    "        if available_important:\n",
    "            df['feature_completeness'] = df[available_important].notna().sum(axis=1) / len(important_features)\n",
    "        else:\n",
    "            df['feature_completeness'] = 0\n",
    "        \n",
    "        # 2. Data density score\n",
    "        f146 = df['f146'] if 'f146' in df.columns else 1\n",
    "        transaction_count = df['transaction_count_last_12h'] if 'transaction_count_last_12h' in df.columns else 1\n",
    "\n",
    "        df['data_density_score'] = (\n",
    "            transaction_count *\n",
    "            f146 *\n",
    "            df['feature_completeness']\n",
    "        ) ** (1/3)\n",
    "        \n",
    "        # 3. Anomaly score (sum of absolute z-scores)\n",
    "        zscore_cols = [col for col in df.columns if col.endswith('_zscore')]\n",
    "        if zscore_cols:\n",
    "            df['anomaly_score'] = df[zscore_cols].abs().sum(axis=1)\n",
    "        else:\n",
    "            df['anomaly_score'] = 0\n",
    "        \n",
    "        # 4. Engagement complexity\n",
    "        engagement_indicators = [f'f{i}' for i in range(14, 22)]\n",
    "        available_engagement = [col for col in engagement_indicators if col in df.columns]\n",
    "\n",
    "        if available_engagement:\n",
    "            df['engagement_complexity'] = df[available_engagement].sum(axis=1)\n",
    "        else:\n",
    "            df['engagement_complexity'] = 0\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = add_meta_features(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac54c0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:52:33.367649Z",
     "iopub.status.busy": "2025-07-20T17:52:33.367421Z",
     "iopub.status.idle": "2025-07-20T17:52:33.527705Z",
     "shell.execute_reply": "2025-07-20T17:52:33.526937Z",
     "shell.execute_reply.started": "2025-07-20T17:52:33.367626Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06de554",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:52:33.528727Z",
     "iopub.status.busy": "2025-07-20T17:52:33.528506Z",
     "iopub.status.idle": "2025-07-20T17:52:33.790289Z",
     "shell.execute_reply": "2025-07-20T17:52:33.789452Z",
     "shell.execute_reply.started": "2025-07-20T17:52:33.528706Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_hyper_temporal_features(train_df, val_df, test_df):\n",
    "    \"\"\"Create advanced temporal velocity and acceleration features\"\"\"\n",
    "    \n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        # 1. Multi-scale velocity ratios\n",
    "        # Building on success of transaction_count_last_12h\n",
    "        if 'transaction_count_last_12h' in df.columns:\n",
    "            df['transaction_velocity_6h_vs_12h'] = (\n",
    "                df.get('transaction_count_last_6h', 0) * 2 / \n",
    "                (df['transaction_count_last_12h'] + 1)\n",
    "            )\n",
    "            df['transaction_acceleration'] = (\n",
    "                df['transaction_velocity_6h_vs_12h'] - 0.5\n",
    "            ) * 2  # Normalized acceleration\n",
    "        \n",
    "        # 2. Click velocity harmonics (building on max/min/avg click velocity)\n",
    "        if all(col in df.columns for col in ['max_click_velocity', 'min_click_velocity', 'avg_click_velocity']):\n",
    "            df['click_velocity_range'] = df['max_click_velocity'] - df['min_click_velocity']\n",
    "            df['click_velocity_stability'] = 1 / (df['click_velocity_range'] + 1)\n",
    "            df['click_velocity_skew'] = (\n",
    "                (df['max_click_velocity'] - df['avg_click_velocity']) /\n",
    "                (df['avg_click_velocity'] - df['min_click_velocity'] + 1e-6)\n",
    "            )\n",
    "        \n",
    "        # 3. Temporal feature derivatives\n",
    "        if 'f209' in df.columns and 'f313' in df.columns:  # 7-day and 14-day CTR\n",
    "            df['ctr_momentum_7_14'] = df['f209'] / (df['f313'] + 1e-6)\n",
    "            df['ctr_jerk'] = df['ctr_momentum_7_14'] - 1  # Rate of change of momentum\n",
    "        \n",
    "        # 4. Complex temporal patterns\n",
    "        if 'time_since_prev_txn_hours' in df.columns:\n",
    "            df['transaction_regularity_score'] = 1 / (\n",
    "                df.groupby('id2')['time_since_prev_txn_hours'].transform('std') + 1\n",
    "            )\n",
    "            df['is_bursty_user'] = (\n",
    "                df['time_since_prev_txn_hours'] < \n",
    "                df['time_since_prev_txn_hours'].quantile(0.1)\n",
    "            ).astype(int)\n",
    "        \n",
    "        # 5. Engagement intensity waves\n",
    "        if 'engagement_intensity' in df.columns and 'f132' in df.columns:\n",
    "            df['engagement_f132_resonance'] = (\n",
    "                df['engagement_intensity'] * df['f132'] / \n",
    "                (df['engagement_intensity'] + df['f132'] + 1e-6)\n",
    "            )  # Harmonic mean\n",
    "            \n",
    "        # 6. Performance floor dynamics\n",
    "        if 'performance_floor' in df.columns and 'worst_day_ctr' in df.columns:\n",
    "            df['floor_to_worst_ratio'] = df['performance_floor'] / (df['worst_day_ctr'] + 1e-6)\n",
    "            df['performance_resilience'] = np.minimum(df['floor_to_worst_ratio'], 10)\n",
    "        \n",
    "    return train_df, val_df, test_df\n",
    "train_df, val_df, test_df = add_hyper_temporal_features(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b7cd2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:52:33.791424Z",
     "iopub.status.busy": "2025-07-20T17:52:33.791144Z",
     "iopub.status.idle": "2025-07-20T17:52:33.952240Z",
     "shell.execute_reply": "2025-07-20T17:52:33.951633Z",
     "shell.execute_reply.started": "2025-07-20T17:52:33.791400Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb56617",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:52:33.953359Z",
     "iopub.status.busy": "2025-07-20T17:52:33.953043Z",
     "iopub.status.idle": "2025-07-20T17:52:44.014615Z",
     "shell.execute_reply": "2025-07-20T17:52:44.013753Z",
     "shell.execute_reply.started": "2025-07-20T17:52:33.953337Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_advanced_statistical_features(train_df, val_df, test_df):\n",
    "    \"\"\"Create sophisticated statistical features building on zscore/percentile success\"\"\"\n",
    "    \n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        # 1. Multi-metric statistical profiles\n",
    "        stat_cols = ['f132', 'f366', 'overall_ctr', 'best_day_ctr']\n",
    "        if all(col in df.columns for col in stat_cols):\n",
    "            # Mahalanobis distance from \"ideal\" customer\n",
    "            from scipy.spatial.distance import mahalanobis\n",
    "            ideal_profile = df[stat_cols].quantile(0.9).values\n",
    "            cov_matrix = df[stat_cols].cov().values\n",
    "            cov_inv = np.linalg.pinv(cov_matrix)\n",
    "            \n",
    "            df['mahalanobis_distance'] = df[stat_cols].apply(\n",
    "                lambda x: mahalanobis(x.values, ideal_profile, cov_inv), axis=1\n",
    "            )\n",
    "            \n",
    "        # 2. Extreme value indicators\n",
    "        if 'f132_zscore' in df.columns and 'overall_ctr_zscore' in df.columns:\n",
    "            df['is_extreme_performer'] = (\n",
    "                (np.abs(df['f132_zscore']) > 2) & \n",
    "                (np.abs(df['overall_ctr_zscore']) > 2)\n",
    "            ).astype(int)\n",
    "            \n",
    "            df['zscore_manhattan'] = (\n",
    "                np.abs(df['f132_zscore']) + \n",
    "                np.abs(df['overall_ctr_zscore'])\n",
    "            )\n",
    "            \n",
    "        # 3. Volatility-adjusted metrics\n",
    "        if 'daily_ctr_volatility' in df.columns:\n",
    "            df['sharpe_ratio'] = df['overall_ctr'] / (df['daily_ctr_volatility'] + 1e-6)\n",
    "            df['volatility_penalty'] = np.exp(-df['daily_ctr_volatility'])\n",
    "            \n",
    "        # 4. Distribution tail features\n",
    "        if 'f366_percentile' in df.columns and 'f132_percentile' in df.columns:\n",
    "            df['is_top_decile_both'] = (\n",
    "                (df['f366_percentile'] > 0.9) & \n",
    "                (df['f132_percentile'] > 0.9)\n",
    "            ).astype(int)\n",
    "            \n",
    "            df['percentile_product'] = df['f366_percentile'] * df['f132_percentile']\n",
    "            df['percentile_gap'] = np.abs(df['f366_percentile'] - df['f132_percentile'])\n",
    "            \n",
    "        # 5. Higher-order moments interaction\n",
    "        if 'metric_kurtosis' in df.columns:\n",
    "            df['kurtosis_risk_score'] = np.clip(df['metric_kurtosis'] / 3, -1, 1)\n",
    "            df['is_fat_tail'] = (df['metric_kurtosis'] > 3).astype(int)\n",
    "        \n",
    "    return train_df, val_df, test_df\n",
    "train_df, val_df, test_df = add_advanced_statistical_features(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710682c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:52:44.015776Z",
     "iopub.status.busy": "2025-07-20T17:52:44.015522Z",
     "iopub.status.idle": "2025-07-20T17:52:44.174929Z",
     "shell.execute_reply": "2025-07-20T17:52:44.174227Z",
     "shell.execute_reply.started": "2025-07-20T17:52:44.015753Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cb8eed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:52:44.176067Z",
     "iopub.status.busy": "2025-07-20T17:52:44.175720Z",
     "iopub.status.idle": "2025-07-20T17:52:44.574688Z",
     "shell.execute_reply": "2025-07-20T17:52:44.574090Z",
     "shell.execute_reply.started": "2025-07-20T17:52:44.176040Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_hyper_customer_features(train_df, val_df, test_df):\n",
    "    \"\"\"Create advanced customer-level features\"\"\"\n",
    "    \n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        # 1. Customer performance stability\n",
    "        if 'max_best_day_ctr_per_id2' in df.columns and 'mean_f132_per_id2' in df.columns:\n",
    "            df['customer_ctr_coefficient_variation'] = (\n",
    "                df.groupby('id2')['best_day_ctr'].transform('std') /\n",
    "                (df['mean_f132_per_id2'] + 1e-6)\n",
    "            )\n",
    "            \n",
    "            df['customer_peak_to_average'] = (\n",
    "                df['max_best_day_ctr_per_id2'] / \n",
    "                (df['mean_f132_per_id2'] + 1e-6)\n",
    "            )\n",
    "            \n",
    "        # 2. Customer engagement patterns\n",
    "        if 'customer_trans_count' in df.columns and 'customer_trans_cv' in df.columns:\n",
    "            df['customer_consistency_score'] = (\n",
    "                df['customer_trans_count'] / \n",
    "                (1 + df['customer_trans_cv'])\n",
    "            )\n",
    "            \n",
    "            df['is_power_user'] = (\n",
    "                (df['customer_trans_count'] > df['customer_trans_count'].quantile(0.8)) &\n",
    "                (df['customer_trans_cv'] < df['customer_trans_cv'].quantile(0.2))\n",
    "            ).astype(int)\n",
    "            \n",
    "        # 3. Customer market position\n",
    "        if 'click_market_share' in df.columns and 'impression_market_share' in df.columns:\n",
    "            df['market_efficiency_ratio'] = (\n",
    "                df['click_market_share'] / \n",
    "                (df['impression_market_share'] + 1e-6)\n",
    "            )\n",
    "            \n",
    "            df['market_dominance_score'] = (\n",
    "                df['click_market_share'] * df['impression_market_share']\n",
    "            ) ** 0.5\n",
    "            \n",
    "        # 4. Customer quality metrics\n",
    "        if all(col in df.columns for col in ['quality_score', 'reliability_score', 'success_rate']):\n",
    "            df['composite_quality_score'] = (\n",
    "                df['quality_score'] * 0.4 +\n",
    "                df['reliability_score'] * 0.3 +\n",
    "                df['success_rate'] * 0.3\n",
    "            )\n",
    "            \n",
    "            df['quality_consistency'] = 1 - df[\n",
    "                ['quality_score', 'reliability_score', 'success_rate']\n",
    "            ].std(axis=1)\n",
    "            \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = add_hyper_customer_features(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da857e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:52:44.575665Z",
     "iopub.status.busy": "2025-07-20T17:52:44.575423Z",
     "iopub.status.idle": "2025-07-20T17:52:44.652397Z",
     "shell.execute_reply": "2025-07-20T17:52:44.651791Z",
     "shell.execute_reply.started": "2025-07-20T17:52:44.575643Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_nonlinear_combinations(train_df, val_df, test_df):\n",
    "    \"\"\"Create non-linear combinations of top features\"\"\"\n",
    "    \n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        # 1. Polynomial interactions of top features\n",
    "        if 'f366_squared' in df.columns and 'f132' in df.columns:\n",
    "            df['f366_f132_product'] = df['f366'] * df['f132']\n",
    "            df['f366_f132_ratio_squared'] = (df['f366'] / (df['f132'] + 1)) ** 2\n",
    "            \n",
    "        # 2. Log-scale interactions\n",
    "        if 'f366_log' in df.columns and 'best_day_ctr_squared' in df.columns:\n",
    "            df['log_squared_interaction'] = df['f366_log'] * np.sqrt(df['best_day_ctr_squared'])\n",
    "            \n",
    "        # 3. Trigonometric combinations\n",
    "        if 'f132' in df.columns and 'f366' in df.columns:\n",
    "            # Map to [0, 2π] range\n",
    "            f132_norm = df['f132'] / (df['f132'].max() + 1) * 2 * np.pi\n",
    "            f366_norm = df['f366'] / (df['f366'].max() + 1) * 2 * np.pi\n",
    "            \n",
    "            df['f132_f366_phase'] = np.sin(f132_norm) * np.cos(f366_norm)\n",
    "            df['f132_f366_magnitude'] = np.sqrt(\n",
    "                np.sin(f132_norm)**2 + np.cos(f366_norm)**2\n",
    "            )\n",
    "            \n",
    "        # 4. Exponential decay combinations\n",
    "        if 'impression_fatigue_score' in df.columns and 'engagement_intensity' in df.columns:\n",
    "            df['fatigue_adjusted_engagement'] = (\n",
    "                df['engagement_intensity'] * \n",
    "                np.exp(-df['impression_fatigue_score'])\n",
    "            )\n",
    "            \n",
    "        # 5. Threshold-based transformations\n",
    "        if 'ctr_vs_median' in df.columns:\n",
    "            df['ctr_sigmoid_transform'] = 1 / (1 + np.exp(-2 * (df['ctr_vs_median'] - 1)))\n",
    "            df['ctr_relu_transform'] = np.maximum(0, df['ctr_vs_median'] - 1)\n",
    "            \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = add_nonlinear_combinations(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4badd8f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:52:44.653407Z",
     "iopub.status.busy": "2025-07-20T17:52:44.653135Z",
     "iopub.status.idle": "2025-07-20T17:53:22.799062Z",
     "shell.execute_reply": "2025-07-20T17:53:22.798313Z",
     "shell.execute_reply.started": "2025-07-20T17:52:44.653383Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_market_intelligence_features(train_df, val_df, test_df):\n",
    "    \"\"\"Create sophisticated market comparison features\"\"\"\n",
    "    \n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        # 1. Market position dynamics\n",
    "        if 'above_market_avg' in df.columns:\n",
    "            df['market_outperformance_degree'] = np.log1p(\n",
    "                np.maximum(0, df['above_market_avg'])\n",
    "            )\n",
    "            \n",
    "            df['consistent_outperformer'] = (\n",
    "                df.groupby('id2')['above_market_avg'].transform(\n",
    "                    lambda x: (x > 0).mean()\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        # 2. Competitive advantage metrics\n",
    "        if 'customer_penetration' in df.columns and 'exposure_efficiency' in df.columns:\n",
    "            df['competitive_advantage_score'] = (\n",
    "                df['customer_penetration'] * df['exposure_efficiency']\n",
    "            )\n",
    "            \n",
    "            df['penetration_efficiency_ratio'] = (\n",
    "                df['customer_penetration'] / \n",
    "                (1 - df['exposure_efficiency'] + 1e-6)\n",
    "            )\n",
    "            \n",
    "        # 3. Market share velocity\n",
    "        if 'click_market_share' in df.columns:\n",
    "            df['market_share_momentum'] = df.groupby('id2')['click_market_share'].transform(\n",
    "                lambda x: x.diff().fillna(0)\n",
    "            )\n",
    "            \n",
    "            df['market_share_acceleration'] = df.groupby('id2')['market_share_momentum'].transform(\n",
    "                lambda x: x.diff().fillna(0)\n",
    "            )\n",
    "            \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = add_market_intelligence_features(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d70a85b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:53:22.800592Z",
     "iopub.status.busy": "2025-07-20T17:53:22.799885Z",
     "iopub.status.idle": "2025-07-20T17:53:22.955851Z",
     "shell.execute_reply": "2025-07-20T17:53:22.955280Z",
     "shell.execute_reply.started": "2025-07-20T17:53:22.800572Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2e48d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:53:22.956732Z",
     "iopub.status.busy": "2025-07-20T17:53:22.956535Z",
     "iopub.status.idle": "2025-07-20T17:53:44.187642Z",
     "shell.execute_reply": "2025-07-20T17:53:44.187048Z",
     "shell.execute_reply.started": "2025-07-20T17:53:22.956717Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "\n",
    "def add_deep_temporal_patterns_gpu(train_df, val_df, test_df):\n",
    "    \"\"\"GPU-accelerated deep temporal patterns\"\"\"\n",
    "\n",
    "    time_windows_dict = {'f29': 30, 'f123': 60, 'f147': 30, 'f201': 14, 'f209': 7}\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, df in zip(['train', 'val', 'test'], [train_df, val_df, test_df]):\n",
    "        df = cudf.from_pandas(df)\n",
    "\n",
    "        # 1. Multi-resolution temporal features\n",
    "        available_cols = [col for col in time_windows_dict.keys() if col in df.columns]\n",
    "        \n",
    "        if len(available_cols) >= 3:\n",
    "            temporal_values = df[available_cols].to_cupy()\n",
    "            df['temporal_consistency'] = 1 / (cp.std(temporal_values, axis=1) + 1)\n",
    "\n",
    "            if len(available_cols) >= 4:\n",
    "                X = cp.array([time_windows_dict[col] for col in available_cols])\n",
    "                X_mean = cp.mean(X)\n",
    "                \n",
    "                y = temporal_values\n",
    "                y_mean = cp.mean(y, axis=1)\n",
    "\n",
    "                numerator = cp.sum((X - X_mean) * (y - y_mean[:, None]), axis=1)\n",
    "                denominator = cp.sum((X - X_mean) ** 2)\n",
    "\n",
    "                slopes = numerator / (denominator + 1e-10)\n",
    "                df['temporal_trend_slope'] = slopes\n",
    "\n",
    "        # 2. Cyclical pattern detection\n",
    "        if 'dining_momentum' in df.columns and 'f125' in df.columns:\n",
    "            df['dining_cycle_strength'] = (\n",
    "                df['dining_momentum'] * cp.sin(df['f125'].to_cupy() / (df['f125'].max() + 1e-10) * 2 * cp.pi)\n",
    "            )\n",
    "        \n",
    "        # 3. Offer response patterns\n",
    "        offer_cols = ['f343', 'f344', 'f345']\n",
    "        if all(col in df.columns for col in offer_cols):\n",
    "            offers = df[offer_cols].to_cupy()\n",
    "            df['offer_response_variance'] = cp.var(offers, axis=1)\n",
    "\n",
    "            row_sums = cp.sum(offers, axis=1)\n",
    "            probs = offers / (row_sums[:, None] + 1e-10)\n",
    "            entropy = -cp.sum(probs * cp.log(probs + 1e-10), axis=1)\n",
    "            df['offer_response_entropy'] = entropy\n",
    "\n",
    "        results[name] = df\n",
    "\n",
    "    # Return in correct order and convert to pandas\n",
    "    return (\n",
    "        results['train'].to_pandas(),\n",
    "        results['val'].to_pandas(),\n",
    "        results['test'].to_pandas()\n",
    "    )\n",
    "\n",
    "\n",
    "train_df, val_df, test_df =  add_deep_temporal_patterns_gpu(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17074bf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:53:44.188916Z",
     "iopub.status.busy": "2025-07-20T17:53:44.188668Z",
     "iopub.status.idle": "2025-07-20T17:53:44.598421Z",
     "shell.execute_reply": "2025-07-20T17:53:44.597787Z",
     "shell.execute_reply.started": "2025-07-20T17:53:44.188893Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_feature_interaction_networks(train_df, val_df, test_df):\n",
    "    \"\"\"Create complex feature interaction networks\"\"\"\n",
    "    \n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        # 1. Performance network features\n",
    "        perf_features = ['success_rate', 'quality_score', 'reliability_score', 'performance_consistency']\n",
    "        available_perf = [f for f in perf_features if f in df.columns]\n",
    "        \n",
    "        if len(available_perf) >= 3:\n",
    "            # Create interaction network\n",
    "            for i, feat1 in enumerate(available_perf):\n",
    "                for feat2 in available_perf[i+1:]:\n",
    "                    df[f'{feat1}_x_{feat2}'] = df[feat1] * df[feat2]\n",
    "                    df[f'{feat1}_div_{feat2}'] = df[feat1] / (df[feat2] + 1e-6)\n",
    "                    \n",
    "        # 2. Click-CTR interaction network  \n",
    "        click_features = ['total_clicks', 'immediate_click_rate', 'quick_click_rate']\n",
    "        ctr_features = ['f313', 'overall_ctr', 'worst_day_ctr']\n",
    "        \n",
    "        for click_feat in click_features:\n",
    "            for ctr_feat in ctr_features:\n",
    "                if click_feat in df.columns and ctr_feat in df.columns:\n",
    "                    df[f'{click_feat}_weighted_by_{ctr_feat}'] = (\n",
    "                        df[click_feat] * df[ctr_feat]\n",
    "                    )\n",
    "                    \n",
    "        # 3. Spend-engagement network\n",
    "        if all(col in df.columns for col in ['f39_ratio', 'f40_ratio', 'f41_ratio']):\n",
    "            df['spend_diversity_index'] = 1 - df[['f39_ratio', 'f40_ratio', 'f41_ratio']].max(axis=1)\n",
    "            df['spend_focus_category'] = df[['f39_ratio', 'f40_ratio', 'f41_ratio']].idxmax(axis=1)\n",
    "            \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = add_feature_interaction_networks(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17941c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:53:44.599492Z",
     "iopub.status.busy": "2025-07-20T17:53:44.599184Z",
     "iopub.status.idle": "2025-07-20T17:53:49.669086Z",
     "shell.execute_reply": "2025-07-20T17:53:49.668499Z",
     "shell.execute_reply.started": "2025-07-20T17:53:44.599466Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_meta_features2(train_df, val_df, test_df):\n",
    "    \"\"\"Create meta-features from existing feature patterns\"\"\"\n",
    "    \n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        # 1. Feature quality indicators\n",
    "        if 'feature_completeness' in df.columns:\n",
    "            df['high_quality_profile'] = (df['feature_completeness'] > 0.8).astype(int)\n",
    "            df['feature_richness'] = df.notna().sum(axis=1) / len(df.columns)\n",
    "            \n",
    "        # 2. Consistency across feature families\n",
    "        feature_families = {\n",
    "            'velocity': ['max_click_velocity', 'min_click_velocity', 'avg_click_velocity'],\n",
    "            'ctr': ['overall_ctr', 'worst_day_ctr', 'best_day_ctr'],\n",
    "            'market': ['click_market_share', 'impression_market_share', 'above_market_avg']\n",
    "        }\n",
    "        \n",
    "        for family_name, family_features in feature_families.items():\n",
    "            available = [f for f in family_features if f in df.columns]\n",
    "            if len(available) >= 2:\n",
    "                df[f'{family_name}_consistency'] = 1 / (df[available].std(axis=1) + 1)\n",
    "                df[f'{family_name}_range'] = df[available].max(axis=1) - df[available].min(axis=1)\n",
    "                \n",
    "        # 3. Anomaly composite scores\n",
    "        anomaly_indicators = ['is_ctr_outlier', 'is_extreme_performer', 'is_fat_tail']\n",
    "        available_anomaly = [f for f in anomaly_indicators if f in df.columns]\n",
    "        if available_anomaly:\n",
    "            df['anomaly_score'] = df[available_anomaly].sum(axis=1)\n",
    "            df['is_super_anomaly'] = (df['anomaly_score'] >= 2).astype(int)\n",
    "            \n",
    "        # 4. Feature interaction strength\n",
    "        if 'f132_zscore' in df.columns and 'f366_percentile' in df.columns:\n",
    "            df['feature_discord'] = np.abs(\n",
    "                df['f132_zscore'] - (df['f366_percentile'] - 0.5) * 4\n",
    "            )\n",
    "            \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "train_df, val_df, test_df = add_meta_features2(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e61351a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:53:49.670093Z",
     "iopub.status.busy": "2025-07-20T17:53:49.669839Z",
     "iopub.status.idle": "2025-07-20T17:53:49.830379Z",
     "shell.execute_reply": "2025-07-20T17:53:49.829651Z",
     "shell.execute_reply.started": "2025-07-20T17:53:49.670075Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f40c7e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:53:49.831551Z",
     "iopub.status.busy": "2025-07-20T17:53:49.831290Z",
     "iopub.status.idle": "2025-07-20T17:53:50.474905Z",
     "shell.execute_reply": "2025-07-20T17:53:50.474126Z",
     "shell.execute_reply.started": "2025-07-20T17:53:49.831533Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_offer_customer_matching(train_df, val_df, test_df):\n",
    "    \"\"\"Create sophisticated offer-customer matching features\"\"\"\n",
    "    \n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        # 1. Offer type affinity scores\n",
    "        if 'preferred_offer_type' in df.columns and 'f111' in df.columns:\n",
    "            # Create binary indicators for each offer type preference\n",
    "            offer_types = df['preferred_offer_type'].unique()\n",
    "            for otype in offer_types:\n",
    "                df[f'prefers_{otype}'] = (df['preferred_offer_type'] == otype).astype(int)\n",
    "                \n",
    "        # 2. Temporal offer matching\n",
    "        if 'offer_start_hour' in df.columns and 'interaction_hour' in df.columns:\n",
    "            df['hour_alignment_score'] = 1 - np.abs(\n",
    "                df['offer_start_hour'] - df['interaction_hour']\n",
    "            ) / 12  # Normalized to [0,1]\n",
    "            \n",
    "            df['is_prime_time_match'] = (\n",
    "                (df['offer_start_hour'].between(9, 11) | df['offer_start_hour'].between(19, 21)) &\n",
    "                (df['interaction_hour'].between(9, 11) | df['interaction_hour'].between(19, 21))\n",
    "            ).astype(int)\n",
    "            \n",
    "        # 3. Offer freshness impact\n",
    "        if 'offer_freshness_category' in df.columns:\n",
    "            freshness_map = {'Fresh': 3, 'Moderate': 2, 'Stale': 1}\n",
    "            df['freshness_score'] = df['offer_freshness_category'].map(freshness_map).fillna(0)\n",
    "            \n",
    "            if 'engagement_intensity' in df.columns:\n",
    "                df['freshness_engagement_product'] = (\n",
    "                    df['freshness_score'] * df['engagement_intensity']\n",
    "                )\n",
    "                \n",
    "        # 4. Discount optimization features\n",
    "        if all(col in df.columns for col in ['discount_above_median_behavioral', 'discount_scarcity']):\n",
    "            df['discount_attractiveness'] = (\n",
    "                df['discount_above_median_behavioral'] * df['discount_scarcity']\n",
    "            )\n",
    "            \n",
    "            if 'is_big_spender' in df.columns:\n",
    "                df['premium_discount_match'] = (\n",
    "                    df['is_big_spender'] * df['discount_above_median_behavioral']\n",
    "                )\n",
    "                \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = add_offer_customer_matching(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2fa594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:53:50.475953Z",
     "iopub.status.busy": "2025-07-20T17:53:50.475719Z",
     "iopub.status.idle": "2025-07-20T17:53:50.481810Z",
     "shell.execute_reply": "2025-07-20T17:53:50.481114Z",
     "shell.execute_reply.started": "2025-07-20T17:53:50.475929Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_final_features(train_df, model, top_150_features, new_features):\n",
    "    \"\"\"Select final features combining top 150 + best new features\"\"\"\n",
    "    \n",
    "    # Get importance for all features\n",
    "    feature_cols = [col for col in train_df.columns if col not in ['y', 'id1', 'id2', 'id3', 'id4', 'id5']]\n",
    "    importance = model.feature_importance(importance_type='gain')\n",
    "    feature_importance = dict(zip(feature_cols, importance))\n",
    "    \n",
    "    # Sort new features by importance\n",
    "    new_feature_importance = {\n",
    "        feat: feature_importance.get(feat, 0) \n",
    "        for feat in new_features if feat in feature_importance\n",
    "    }\n",
    "    \n",
    "    # Select top 50 new features\n",
    "    top_new_features = sorted(\n",
    "        new_feature_importance.items(), \n",
    "        key=lambda x: x[1], \n",
    "        reverse=True\n",
    "    )[:50]\n",
    "    \n",
    "    # Combine with original top 150\n",
    "    final_features = list(set(top_150_features + [f[0] for f in top_new_features]))\n",
    "    \n",
    "    print(f\"Final feature set: {len(final_features)} features\")\n",
    "    print(f\"Top 10 new features added:\")\n",
    "    for feat, imp in top_new_features[:10]:\n",
    "        print(f\"  {feat}: {imp:.2f}\")\n",
    "        \n",
    "    return final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e01393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:53:50.482814Z",
     "iopub.status.busy": "2025-07-20T17:53:50.482568Z",
     "iopub.status.idle": "2025-07-20T17:53:50.644437Z",
     "shell.execute_reply": "2025-07-20T17:53:50.643713Z",
     "shell.execute_reply.started": "2025-07-20T17:53:50.482791Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3270ea97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:53:50.645513Z",
     "iopub.status.busy": "2025-07-20T17:53:50.645271Z",
     "iopub.status.idle": "2025-07-20T17:53:50.655121Z",
     "shell.execute_reply": "2025-07-20T17:53:50.654526Z",
     "shell.execute_reply.started": "2025-07-20T17:53:50.645491Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_df.shape, \n",
    "val_df.shape,\n",
    "test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2abf922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:53:50.656111Z",
     "iopub.status.busy": "2025-07-20T17:53:50.655864Z",
     "iopub.status.idle": "2025-07-20T17:53:50.797534Z",
     "shell.execute_reply": "2025-07-20T17:53:50.796898Z",
     "shell.execute_reply.started": "2025-07-20T17:53:50.656086Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9040d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:53:50.798398Z",
     "iopub.status.busy": "2025-07-20T17:53:50.798204Z",
     "iopub.status.idle": "2025-07-20T17:54:13.028772Z",
     "shell.execute_reply": "2025-07-20T17:54:13.028205Z",
     "shell.execute_reply.started": "2025-07-20T17:53:50.798383Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data_for_lgb(train_df: pd.DataFrame, val_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Prepare data for LightGBM by converting object columns to numeric\n",
    "    \"\"\"\n",
    "    train_df_copy = train_df.copy()\n",
    "    val_df_copy = val_df.copy()\n",
    "    \n",
    "    # Get feature columns (exclude target 'y')\n",
    "    feature_cols = [col for col in train_df_copy.columns if col not in ['id1', 'id3', 'id4', 'id5', 'id6', 'id7', 'id8','id9', 'id10', 'id11', 'id12', 'id13','id2','y' 'pred']]\n",
    "    \n",
    "    # Find object columns\n",
    "    object_cols = []\n",
    "    for col in feature_cols:\n",
    "        if train_df_copy[col].dtype == 'object':\n",
    "            object_cols.append(col)\n",
    "    \n",
    "    print(f\"Found {len(object_cols)} object columns to convert:\")\n",
    "    print(object_cols)\n",
    "    \n",
    "    # Convert object columns to numeric\n",
    "    for col in object_cols:\n",
    "        print(f\"Converting column: {col}\")\n",
    "        \n",
    "        # Combine train and val data for consistent encoding\n",
    "        combined_data = pd.concat([train_df_copy[col], val_df_copy[col]], axis=0)\n",
    "        \n",
    "        # Try to convert to numeric first (in case they're numeric strings)\n",
    "        try:\n",
    "            combined_numeric = pd.to_numeric(combined_data, errors='coerce')\n",
    "            if combined_numeric.notna().sum() > len(combined_data) * 0.8:  # If 80%+ can be converted\n",
    "                train_df_copy[col] = pd.to_numeric(train_df_copy[col], errors='coerce')\n",
    "                val_df_copy[col] = pd.to_numeric(val_df_copy[col], errors='coerce')\n",
    "                print(f\"  -> Converted to numeric\")\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # If not numeric, use label encoding\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le = LabelEncoder()\n",
    "        \n",
    "        # Fit on combined data to ensure consistent encoding\n",
    "        combined_data_filled = combined_data.fillna('missing')\n",
    "        le.fit(combined_data_filled)\n",
    "        \n",
    "        # Transform train and val data\n",
    "        train_filled = train_df_copy[col].fillna('missing')\n",
    "        val_filled = val_df_copy[col].fillna('missing')\n",
    "        \n",
    "        train_df_copy[col] = le.transform(train_filled)\n",
    "        val_df_copy[col] = le.transform(val_filled)\n",
    "        \n",
    "        print(f\"  -> Label encoded ({len(le.classes_)} unique values)\")\n",
    "    \n",
    "    # Convert any remaining object columns to numeric\n",
    "    for col in feature_cols:\n",
    "        if train_df_copy[col].dtype == 'object':\n",
    "            train_df_copy[col] = pd.to_numeric(train_df_copy[col], errors='coerce').fillna(0)\n",
    "            val_df_copy[col] = pd.to_numeric(val_df_copy[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Check final dtypes\n",
    "    print(\"\\nFinal data types:\")\n",
    "    feature_dtypes = train_df_copy[feature_cols].dtypes\n",
    "    object_remaining = feature_dtypes[feature_dtypes == 'object']\n",
    "    if len(object_remaining) > 0:\n",
    "        print(f\"WARNING: Still have object columns: {object_remaining.index.tolist()}\")\n",
    "    else:\n",
    "        print(\"✓ All feature columns are numeric\")\n",
    "    \n",
    "    return train_df_copy, val_df_copy\n",
    "\n",
    "train_df, val_df = prepare_data_for_lgb(train_df, val_df)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc078b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:54:13.029753Z",
     "iopub.status.busy": "2025-07-20T17:54:13.029546Z",
     "iopub.status.idle": "2025-07-20T17:55:22.937740Z",
     "shell.execute_reply": "2025-07-20T17:55:22.937152Z",
     "shell.execute_reply.started": "2025-07-20T17:54:13.029737Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "import shap\n",
    "import gc\n",
    "\n",
    "# Step 1: LightGBM GPU Feature Importance\n",
    "def get_lgb_gpu_importance(train_df, feature_cols):\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': -1,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'min_data_in_leaf': 100,\n",
    "        'verbosity': -1,\n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    dtrain = lgb.Dataset(train_df[feature_cols].astype(np.float32), label=train_df['y'])\n",
    "    \n",
    "    model = lgb.train(params, dtrain, num_boost_round=300)\n",
    "    \n",
    "    imp_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': model.feature_importance(importance_type='gain')\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    top_features = imp_df.head(300)['feature'].tolist()\n",
    "    \n",
    "    del dtrain, model\n",
    "    gc.collect()\n",
    "    \n",
    "    return top_features, imp_df\n",
    "\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "def cupy_rankdata(array):\n",
    "    \"\"\"Cupy's version of rankdata with average ranking for ties and NaN-safe.\"\"\"\n",
    "    n = array.size\n",
    "    ranks = cp.zeros(n, dtype=cp.float32)\n",
    "\n",
    "    nan_mask = cp.isnan(array)\n",
    "    not_nan = ~nan_mask\n",
    "\n",
    "    valid_vals = array[not_nan]\n",
    "    sorter = cp.argsort(valid_vals)\n",
    "    inv = cp.empty_like(sorter)\n",
    "    inv[sorter] = cp.arange(len(valid_vals))\n",
    "\n",
    "    sorted_vals = valid_vals[sorter]\n",
    "    obs = cp.concatenate((cp.array([True], dtype=cp.bool_), (sorted_vals[1:] != sorted_vals[:-1])))\n",
    "\n",
    "    dense_rank = cp.cumsum(obs).astype(cp.float32)\n",
    "\n",
    "    counts = cp.bincount(dense_rank.astype(cp.int32))\n",
    "    avg_ranks = cp.cumsum(counts) / counts\n",
    "    ranks_non_nan = avg_ranks[dense_rank.astype(cp.int32) - 1]\n",
    "\n",
    "    ranks[not_nan] = ranks_non_nan\n",
    "    ranks[nan_mask] = cp.nan\n",
    "\n",
    "    return ranks\n",
    "\n",
    "def gpu_mutual_info_proxy(train_df, feature_cols):\n",
    "    \"\"\"\n",
    "    GPU-based mutual information proxy using Spearman rank correlation.\n",
    "    **NOW FULLY NULL-SAFE**\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Step 2: GPU Mutual Info Approximation with Null Handling...\")\n",
    "\n",
    "    # Convert to cudf\n",
    "    X_cudf = cudf.DataFrame(train_df[feature_cols].astype(np.float32))\n",
    "    y_cudf = cudf.Series(train_df['y'].astype(np.int8))\n",
    "\n",
    "    # Fill NaNs before .to_cupy() to avoid cudf error\n",
    "    X_cudf = X_cudf.fillna(X_cudf.median())\n",
    "    y_cudf = y_cudf.fillna(y_cudf.median())\n",
    "\n",
    "    # Convert to cupy\n",
    "    X_cp = X_cudf.to_cupy()\n",
    "    y_cp = y_cudf.to_cupy()\n",
    "\n",
    "    # Rank transform\n",
    "    X_rank = cp.zeros_like(X_cp)\n",
    "    for i in range(X_cp.shape[1]):\n",
    "        X_rank[:, i] = cupy_rankdata(X_cp[:, i])\n",
    "\n",
    "    y_rank = cupy_rankdata(y_cp)\n",
    "\n",
    "    # Center ranks\n",
    "    X_rank -= cp.mean(X_rank, axis=0)\n",
    "    y_rank -= cp.mean(y_rank)\n",
    "\n",
    "    # Spearman correlation\n",
    "    numerator = cp.sum(X_rank * y_rank[:, None], axis=0)\n",
    "    denominator = cp.sqrt(cp.sum(X_rank ** 2, axis=0)) * cp.sqrt(cp.sum(y_rank ** 2))\n",
    "\n",
    "    spearman_corr = numerator / (denominator + 1e-10)\n",
    "\n",
    "    # MI proxy = abs(Spearman correlation)\n",
    "    mi_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'mi_score': cp.abs(spearman_corr).get()\n",
    "    }).sort_values(by='mi_score', ascending=False)\n",
    "\n",
    "    del X_cudf, y_cudf, X_cp, y_cp, X_rank, y_rank\n",
    "    gc.collect()\n",
    "\n",
    "    return mi_df\n",
    "\n",
    "# Step 3: SHAP Feature Selection (Interaction-aware)\n",
    "def get_shap_importance(train_df, feature_cols, n_samples=10000):\n",
    "    sample_df = train_df.sample(n=min(n_samples, len(train_df)), random_state=42)\n",
    "    \n",
    "    X_sample = sample_df[feature_cols].astype(np.float32)\n",
    "    y_sample = sample_df['y'].astype(np.int8)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': -1,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'min_data_in_leaf': 100,\n",
    "        'verbosity': -1,\n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    dtrain = lgb.Dataset(X_sample, label=y_sample)\n",
    "    model = lgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_sample)[1]  # Binary classification -> index 1 is for positive class\n",
    "\n",
    "    shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "    shap_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'shap_value': shap_importance\n",
    "    }).sort_values(by='shap_value', ascending=False)\n",
    "\n",
    "    del X_sample, y_sample, dtrain, model, explainer, shap_values\n",
    "    gc.collect()\n",
    "    \n",
    "    return shap_df\n",
    "\n",
    "# Step 4: Run the pipeline\n",
    "def run_feature_selection_pipeline(train_df, top_k=200):\n",
    "    print(\"Extracting feature columns...\")\n",
    "    feature_cols = [col for col in train_df.columns if col not in ['id1', 'id2', 'id3', 'id4', 'id5', 'y', 'pred'] and not col.endswith('_dt')]\n",
    "\n",
    "    print(\"Step 1: LightGBM GPU Importance...\")\n",
    "    lgb_top_features, lgb_df = get_lgb_gpu_importance(train_df, feature_cols)\n",
    "\n",
    "    print(\"Step 2: Mutual Information...\")\n",
    "    # Step 2: Mutual Info Proxy GPU\n",
    "    mi_df = gpu_mutual_info_proxy(train_df, feature_cols)\n",
    "\n",
    "\n",
    "    print(\"Step 3: SHAP Importance...\")\n",
    "    shap_df = get_shap_importance(train_df, feature_cols)\n",
    "\n",
    "    print(\"Step 4: Aggregate & Select Top Features...\")\n",
    "\n",
    "    # Normalize scores for fair aggregation\n",
    "    lgb_df['norm_imp'] = lgb_df['importance'] / (lgb_df['importance'].sum() + 1e-10)\n",
    "    mi_df['norm_mi'] = mi_df['mi_score'] / (mi_df['mi_score'].sum() + 1e-10)\n",
    "    shap_df['norm_shap'] = shap_df['shap_value'] / (shap_df['shap_value'].sum() + 1e-10)\n",
    "\n",
    "    # Merge scores\n",
    "    agg = lgb_df[['feature', 'norm_imp']].merge(\n",
    "        mi_df[['feature', 'norm_mi']], on='feature', how='outer'\n",
    "    ).merge(\n",
    "        shap_df[['feature', 'norm_shap']], on='feature', how='outer'\n",
    "    ).fillna(0)\n",
    "\n",
    "    # Weighted ensemble of scores\n",
    "    agg['total_score'] = 0.4 * agg['norm_imp'] + 0.3 * agg['norm_mi'] + 0.3 * agg['norm_shap']\n",
    "    agg = agg.sort_values('total_score', ascending=False)\n",
    "\n",
    "    selected_features = agg.head(top_k)['feature'].tolist()\n",
    "\n",
    "    print(f\"Selected top {top_k} features.\")\n",
    "    \n",
    "    return selected_features, agg\n",
    "\n",
    "\n",
    "selected_features, feature_score_df = run_feature_selection_pipeline(train_df, top_k=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f034b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:55:22.938897Z",
     "iopub.status.busy": "2025-07-20T17:55:22.938384Z",
     "iopub.status.idle": "2025-07-20T17:55:22.944680Z",
     "shell.execute_reply": "2025-07-20T17:55:22.944104Z",
     "shell.execute_reply.started": "2025-07-20T17:55:22.938877Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc82749",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:55:22.945680Z",
     "iopub.status.busy": "2025-07-20T17:55:22.945433Z",
     "iopub.status.idle": "2025-07-20T17:55:23.152406Z",
     "shell.execute_reply": "2025-07-20T17:55:23.151629Z",
     "shell.execute_reply.started": "2025-07-20T17:55:22.945660Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d6f7f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:55:23.153560Z",
     "iopub.status.busy": "2025-07-20T17:55:23.153300Z",
     "iopub.status.idle": "2025-07-20T17:55:24.227259Z",
     "shell.execute_reply": "2025-07-20T17:55:24.226623Z",
     "shell.execute_reply.started": "2025-07-20T17:55:23.153538Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_cols = ['id1', 'id2', 'id3', 'id4', 'id5']\n",
    "\n",
    "# For Train\n",
    "train_filtered = train_df[selected_features + id_cols + ['y']].copy()\n",
    "\n",
    "# For Validation\n",
    "val_filtered = val_df[selected_features + id_cols + ['y']].copy()\n",
    "\n",
    "# For Test (no 'y')\n",
    "test_filtered = test_df[selected_features + id_cols].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb6cb7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T17:55:24.228210Z",
     "iopub.status.busy": "2025-07-20T17:55:24.227976Z",
     "iopub.status.idle": "2025-07-20T17:55:24.232936Z",
     "shell.execute_reply": "2025-07-20T17:55:24.232215Z",
     "shell.execute_reply.started": "2025-07-20T17:55:24.228185Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370273de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T18:14:06.074472Z",
     "iopub.status.busy": "2025-07-20T18:14:06.074234Z",
     "iopub.status.idle": "2025-07-20T18:14:13.479805Z",
     "shell.execute_reply": "2025-07-20T18:14:13.479210Z",
     "shell.execute_reply.started": "2025-07-20T18:14:06.074454Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_fast_lag_features(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame,\n",
    "                           target_col: str = 'y') -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create essential lag features optimized for speed and large imbalanced datasets.\n",
    "    Focus on features that provide maximum signal with minimum computation.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_df, val_df, test_df: DataFrames with time series data\n",
    "    - target_col: Name of target column (to avoid data leakage)\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of enhanced DataFrames with essential lag features\n",
    "    \"\"\"\n",
    "    print(\"=== Creating Fast Essential Lag Features ===\")\n",
    "    \n",
    "    # Create copies\n",
    "    train_df = train_df.copy()\n",
    "    val_df = val_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    # Ensure datetime\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        df['id4'] = pd.to_datetime(df['id4'], errors='coerce')\n",
    "    \n",
    "    # Define ONLY the most important features for lag creation\n",
    "    # Focus on behavioral signals that are most predictive\n",
    "    essential_features = [\n",
    "        'f132',           # Core transaction amount\n",
    "        'best_day_ctr',   # Key performance metric\n",
    "        'overall_ctr',    # Overall engagement\n",
    "    ]\n",
    "    \n",
    "    # Add GPU features if they exist (these are usually high-value)\n",
    "    gpu_features = ['customer_avg_trans', 'customer_trans_count']\n",
    "    for feature in gpu_features:\n",
    "        if feature in train_df.columns:\n",
    "            essential_features.append(feature)\n",
    "    \n",
    "    # Filter to only existing features\n",
    "    # This line should be added to filter out the target column:\n",
    "    available_features = [f for f in essential_features \n",
    "                     if f in train_df.columns and f != target_col]\n",
    "    print(f\"Creating lag features for {len(available_features)} high-value features: {available_features}\")\n",
    "    \n",
    "    def create_minimal_lags(df):\n",
    "        \"\"\"Create only the most essential lag features for speed\"\"\"\n",
    "        df_enhanced = df.copy()\n",
    "        df_enhanced = df_enhanced.sort_values(['id2', 'id4'])\n",
    "        \n",
    "        print(\"   Creating essential lag features...\")\n",
    "        \n",
    "        # 1. ONLY most critical lags (1, 7 days)\n",
    "        critical_lags = [1, 7]  # Reduced from [1, 2, 3, 7, 14]\n",
    "        \n",
    "        for feature in available_features:\n",
    "            # Basic lags - only most important ones\n",
    "            for lag in critical_lags:\n",
    "                df_enhanced[f'{feature}_lag_{lag}'] = df_enhanced.groupby('id2')[feature].shift(lag)\n",
    "            \n",
    "            # Only short-term rolling mean (most predictive)\n",
    "            df_enhanced[f'{feature}_roll3'] = (\n",
    "                df_enhanced.groupby('id2')[feature]\n",
    "                .rolling(window=3, min_periods=1)\n",
    "                .mean()\n",
    "                .reset_index(level=0, drop=True)\n",
    "            )\n",
    "            \n",
    "            # Recent change (most important momentum signal)\n",
    "            df_enhanced[f'{feature}_diff1'] = df_enhanced.groupby('id2')[feature].diff(1)\n",
    "            \n",
    "            # Current vs recent average (key signal for imbalanced data)\n",
    "            df_enhanced[f'{feature}_vs_avg'] = (\n",
    "                df_enhanced[feature] - df_enhanced[f'{feature}_roll3']\n",
    "            )\n",
    "        \n",
    "        print(\"   Creating cross-feature ratios...\")\n",
    "        # 2. ONLY most important cross-feature interactions\n",
    "        if 'best_day_ctr' in df_enhanced.columns and 'overall_ctr' in df_enhanced.columns:\n",
    "            df_enhanced['ctr_ratio_lag1'] = (\n",
    "                df_enhanced['best_day_ctr_lag_1'] / (df_enhanced['overall_ctr_lag_1'] + 1e-6)\n",
    "            )\n",
    "        \n",
    "        if 'f132' in df_enhanced.columns:\n",
    "            # Transaction amount momentum\n",
    "            df_enhanced['f132_momentum'] = (\n",
    "                df_enhanced['f132'] / (df_enhanced['f132_roll3'] + 1e-6)\n",
    "            )\n",
    "        \n",
    "        return df_enhanced\n",
    "    \n",
    "    def create_imbalanced_features(df):\n",
    "        \"\"\"Create features specifically useful for imbalanced datasets\"\"\"\n",
    "        df_enhanced = df.copy()\n",
    "        \n",
    "        print(\"   Creating imbalanced-specific features...\")\n",
    "        \n",
    "        # Customer behavior consistency (helps identify anomalies in imbalanced data)\n",
    "        for feature in available_features:\n",
    "            if f'{feature}_roll3' in df_enhanced.columns:\n",
    "                # Coefficient of variation (stability measure)\n",
    "                rolling_std = (\n",
    "                    df_enhanced.groupby('id2')[feature]\n",
    "                    .rolling(window=3, min_periods=2)\n",
    "                    .std()\n",
    "                    .reset_index(level=0, drop=True)\n",
    "                )\n",
    "                df_enhanced[f'{feature}_stability'] = (\n",
    "                    rolling_std / (df_enhanced[f'{feature}_roll3'] + 1e-6)\n",
    "                )\n",
    "        \n",
    "        # Recent activity level (important for rare events)\n",
    "        if 'f132' in available_features:\n",
    "            df_enhanced['recent_activity'] = (\n",
    "                df_enhanced.groupby('id2')['f132']\n",
    "                .rolling(window=3, min_periods=1)\n",
    "                .count()\n",
    "                .reset_index(level=0, drop=True)\n",
    "            )\n",
    "        \n",
    "        return df_enhanced\n",
    "    \n",
    "    def fast_fill_missing(df):\n",
    "        \"\"\"Fast missing value filling optimized for large datasets\"\"\"\n",
    "        # Use median filling (faster than forward fill for large data)\n",
    "        lag_cols = [col for col in df.columns if any(x in col for x in ['_lag_', '_roll', '_diff', '_vs_', '_stability', '_momentum'])]\n",
    "        \n",
    "        for col in lag_cols:\n",
    "            if col.endswith('_diff1'):\n",
    "                df[col] = df[col].fillna(0)  # No change\n",
    "            elif 'stability' in col or 'momentum' in col:\n",
    "                df[col] = df[col].fillna(1)  # Neutral values\n",
    "            else:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Apply to all datasets\n",
    "    print(\"\\nProcessing training data...\")\n",
    "    train_enhanced = create_minimal_lags(train_df)\n",
    "    train_enhanced = create_imbalanced_features(train_enhanced)\n",
    "    train_enhanced = fast_fill_missing(train_enhanced)\n",
    "    \n",
    "    print(\"Processing validation data...\")\n",
    "    val_enhanced = create_minimal_lags(val_df)\n",
    "    val_enhanced = create_imbalanced_features(val_enhanced)\n",
    "    val_enhanced = fast_fill_missing(val_enhanced)\n",
    "    \n",
    "    print(\"Processing test data...\")\n",
    "    test_enhanced = create_minimal_lags(test_df)\n",
    "    test_enhanced = create_imbalanced_features(test_enhanced)\n",
    "    test_enhanced = fast_fill_missing(test_enhanced)\n",
    "    \n",
    "    # Summary\n",
    "    original_cols = len(train_df.columns)\n",
    "    new_cols = len(train_enhanced.columns) - original_cols\n",
    "    \n",
    "    print(f\"\\n=== Fast Lag Feature Creation Complete ===\")\n",
    "    print(f\"New features created: {new_cols} (optimized for speed)\")\n",
    "    print(f\"Training shape: {train_enhanced.shape}\")\n",
    "    print(f\"Features per base feature: ~{new_cols // len(available_features)} (vs ~50+ in full version)\")\n",
    "    \n",
    "    return train_enhanced, val_enhanced, test_enhanced\n",
    "\n",
    "\n",
    "def create_ultra_fast_features(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Ultra-fast version with only the most critical features for very large datasets.\n",
    "    Creates ~3-5 features per base feature instead of 50+.\n",
    "    \"\"\"\n",
    "    print(\"=== Creating Ultra-Fast Critical Features ===\")\n",
    "    \n",
    "    train_df = train_df.copy()\n",
    "    val_df = val_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    # Ensure datetime\n",
    "    for df in [train_df, val_df, test_df]:\n",
    "        df['id4'] = pd.to_datetime(df['id4'], errors='coerce')\n",
    "    \n",
    "    # ONLY the most critical features\n",
    "    critical_features = ['f132', 'best_day_ctr']\n",
    "    available = [f for f in critical_features if f in train_df.columns]\n",
    "    \n",
    "    def create_critical_only(df):\n",
    "        df_enhanced = df.copy()\n",
    "        df_enhanced = df_enhanced.sort_values(['id2', 'id4'])\n",
    "        \n",
    "        for feature in available:\n",
    "            # Only 1-day lag (most important)\n",
    "            df_enhanced[f'{feature}_lag1'] = df_enhanced.groupby('id2')[feature].shift(1)\n",
    "            \n",
    "            # Only 3-day rolling average\n",
    "            df_enhanced[f'{feature}_avg3'] = (\n",
    "                df_enhanced.groupby('id2')[feature]\n",
    "                .rolling(window=3, min_periods=1)\n",
    "                .mean()\n",
    "                .reset_index(level=0, drop=True)\n",
    "            )\n",
    "            \n",
    "            # Current vs recent (key momentum signal)\n",
    "            df_enhanced[f'{feature}_momentum'] = (\n",
    "                df_enhanced[feature] / (df_enhanced[f'{feature}_avg3'] + 1e-6)\n",
    "            )\n",
    "        \n",
    "        # Fill missing with median (fastest)\n",
    "        new_cols = [col for col in df_enhanced.columns if col not in df.columns]\n",
    "        for col in new_cols:\n",
    "            df_enhanced[col] = df_enhanced[col].fillna(df_enhanced[col].median())\n",
    "        \n",
    "        return df_enhanced\n",
    "    \n",
    "    # Process all datasets\n",
    "    train_enhanced = create_critical_only(train_df)\n",
    "    val_enhanced = create_critical_only(val_df)\n",
    "    test_enhanced = create_critical_only(test_df)\n",
    "    \n",
    "    new_features = len(train_enhanced.columns) - len(train_df.columns)\n",
    "    print(f\"Created {new_features} ultra-fast features\")\n",
    "    print(f\"Training shape: {train_enhanced.shape}\")\n",
    "    \n",
    "    return train_enhanced, val_enhanced, test_enhanced\n",
    "\n",
    "train_filtered, val_filtered, test_filtered = create_fast_lag_features(train_filtered, val_filtered, test_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853cd0c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T18:14:17.757345Z",
     "iopub.status.busy": "2025-07-20T18:14:17.757057Z",
     "iopub.status.idle": "2025-07-20T18:14:17.774350Z",
     "shell.execute_reply": "2025-07-20T18:14:17.773421Z",
     "shell.execute_reply.started": "2025-07-20T18:14:17.757323Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Delete old datasets explicitly\n",
    "del train_df\n",
    "del val_df\n",
    "del test_df\n",
    "\n",
    "# Run garbage collection to free memory\n",
    "gc.collect()\n",
    "\n",
    "# Optional: Clear CuPy GPU memory if you're using it\n",
    "import cupy as cp\n",
    "cp._default_memory_pool.free_all_blocks()\n",
    "\n",
    "print(\"Old datasets deleted and memory freed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64f3efd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T18:14:19.759201Z",
     "iopub.status.busy": "2025-07-20T18:14:19.758930Z",
     "iopub.status.idle": "2025-07-20T18:14:19.776256Z",
     "shell.execute_reply": "2025-07-20T18:14:19.775451Z",
     "shell.execute_reply.started": "2025-07-20T18:14:19.759151Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "\n",
    "def train_model(train_df: pd.DataFrame, val_df: pd.DataFrame) -> lgb.Booster:\n",
    "    \"\"\"Train LightGBM model for ranking with target 'y', calculate ranking metrics,\n",
    "    and dynamically select the optimal threshold based on F1-score.\"\"\"\n",
    "    \n",
    "    # Get feature columns (exclude target column 'y', ID columns except 'id2', and date columns)\n",
    "    feature_cols = [col for col in train_df.columns if col not in ['id1','id2','id3', 'id4', 'id5', 'id6', 'id7', 'id8','id9', 'id10', 'id11', 'id12', 'id13','y', 'pred'] and not col.endswith('_dt')]\n",
    "    \n",
    "    # Prepare data and get categorical columns\n",
    "    train_df, val_df = prepare_data_for_lgb(train_df, val_df)\n",
    "    \n",
    "    print(f\"Training data: {len(train_df)} rows, {len(feature_cols)} features\")\n",
    "    print(f\"Validation data: {len(val_df)} rows\")\n",
    "    # print(f\"Categorical columns passed to LightGBM: {categorical_cols}\")\n",
    "    \n",
    "    # Check target distribution\n",
    "    train_target_dist = train_df['y'].value_counts().sort_index()\n",
    "    val_target_dist = val_df['y'].value_counts().sort_index()\n",
    "    print(f\"Train target distribution: {train_target_dist.to_dict()}\")\n",
    "    print(f\"Val target distribution: {val_target_dist.to_dict()}\")\n",
    "    \n",
    "    # Calculate scale_pos_weight for imbalanced data\n",
    "    pos_samples = len(train_df[train_df['y'] == 1])\n",
    "    neg_samples = len(train_df[train_df['y'] == 0])\n",
    "    scale_pos_weight = neg_samples / pos_samples if pos_samples > 0 else 1.0\n",
    "    \n",
    "    # Optimized LightGBM parameters for ranking\n",
    "    params = {\n",
    "    'objective': 'lambdarank',\n",
    "    'metric': 'map',\n",
    "    'eval_at': [7],\n",
    "    'lambdarank_truncation_level': 7,\n",
    "    'lambdarank_norm': True,\n",
    "\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 63,                # Restore leaves for more splits\n",
    "    'learning_rate': 0.03,           # Increase LR for faster learning\n",
    "    'max_depth': -1,                 # Allow deeper trees\n",
    "\n",
    "    'feature_fraction': 0.8,         # Slightly more features per split\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "\n",
    "    'lambda_l1': 0.1,                # Relax L1\n",
    "    'lambda_l2': 0.1,                # Relax L2\n",
    "    'min_data_in_leaf': 50,          # Reduce from 200 to 50\n",
    "    'min_gain_to_split': 0.0,        # Allow any positive gain\n",
    "    'max_bin': 255,\n",
    "\n",
    "    'force_col_wise': True,\n",
    "    'num_threads': -1,\n",
    "    'verbosity': 1,\n",
    "    'seed': 42,\n",
    "    }\n",
    "\n",
    "    print(f\"Scale pos weight: {scale_pos_weight:.4f}\")\n",
    "    \n",
    "    # Sort dataframes by customer for proper grouping\n",
    "    train_df = train_df.sort_values('id2').reset_index(drop=True)\n",
    "    val_df = val_df.sort_values('id2').reset_index(drop=True)\n",
    "    \n",
    "    # Create group information for ranking\n",
    "    train_groups = train_df.groupby('id2').size().values\n",
    "    val_groups = val_df.groupby('id2').size().values\n",
    "    \n",
    "    print(f\"Train groups: {len(train_groups)} customers, avg {train_groups.mean():.1f} offers per customer\")\n",
    "    print(f\"Val groups: {len(val_groups)} customers, avg {val_groups.mean():.1f} offers per customer\")\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"Creating LightGBM datasets...\")\n",
    "    dtrain = lgb.Dataset(train_df[feature_cols], label=train_df['y'], group=train_groups, free_raw_data=False)\n",
    "    dval = lgb.Dataset(val_df[feature_cols], label=val_df['y'], group=val_groups, reference=dtrain, free_raw_data=False)\n",
    "    \n",
    "    dtrain.construct()\n",
    "    dval.construct()\n",
    "    \n",
    "    print(f\"Training dataset created: {dtrain.num_data()} samples\")\n",
    "    print(f\"Validation dataset created: {dval.num_data()} samples\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Starting LightGBM training...\")\n",
    "    start_time = time.time()\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=3000,\n",
    "        valid_sets=[dtrain, dval],\n",
    "        valid_names=['train', 'val'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=200, verbose=True),\n",
    "            lgb.log_evaluation(period=50)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    print(f\"Best iteration: {model.best_iteration}\")\n",
    "    print(f\"Best score: {model.best_score}\")\n",
    "    \n",
    "    # Calculate ranking scores\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RANKING EVALUATION (Using Ranking Scores)\")\n",
    "    print(\"=\"*50)\n",
    "    gc.collect()\n",
    "    # Predictions on training set (ranking scores)\n",
    "    train_pred = model.predict(train_df[feature_cols], num_iteration=model.best_iteration)\n",
    "    train_auc = roc_auc_score(train_df['y'], train_pred)\n",
    "    gc.collect()\n",
    "    # Predictions on validation set (ranking scores)\n",
    "    val_pred = model.predict(val_df[feature_cols], num_iteration=model.best_iteration)\n",
    "    val_auc = roc_auc_score(val_df['y'], val_pred)\n",
    "    gc.collect()\n",
    "    print(f\"Training AUC: {train_auc:.6f}\")\n",
    "    print(f\"Validation AUC: {val_auc:.6f}\")\n",
    "    print(f\"AUC Difference (Train - Val): {train_auc - val_auc:.6f}\")\n",
    "    \n",
    "    # Dynamic threshold selection based on F1-score\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DYNAMIC THRESHOLD SELECTION (Maximizing F1-Score)\")\n",
    "    print(\"=\"*50)\n",
    "    gc.collect()\n",
    "    thresholds = np.arange(0.1, 0.91, 0.01)\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0.0\n",
    "    best_precision = 0.0\n",
    "    best_recall = 0.0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        val_pred_binary = (val_pred > threshold).astype(int)\n",
    "        f1 = f1_score(val_df['y'], val_pred_binary)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_precision = precision_score(val_df['y'], val_pred_binary)\n",
    "            best_recall = recall_score(val_df['y'], val_pred_binary)\n",
    "    \n",
    "    print(f\"Optimal threshold: {best_threshold:.2f}\")\n",
    "    print(f\"Best F1-Score: {best_f1:.6f}\")\n",
    "    print(f\"Precision at optimal threshold: {best_precision:.6f}\")\n",
    "    print(f\"Recall at optimal threshold: {best_recall:.6f}\")\n",
    "    gc.collect()\n",
    "    # Generate binary predictions using the optimal threshold\n",
    "    val_pred_binary = (val_pred > best_threshold).astype(int)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"CLASSIFICATION REPORT (Validation Set, Threshold={best_threshold:.2f})\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(val_df['y'], val_pred_binary))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix (Validation Set):\")\n",
    "    cm = confusion_matrix(val_df['y'], val_pred_binary)\n",
    "    print(cm)\n",
    "    gc.collect()\n",
    "    # Store metrics in model object for later access\n",
    "    model.train_auc = train_auc\n",
    "    model.val_auc = val_auc\n",
    "    model.val_precision = best_precision\n",
    "    model.val_recall = best_recall\n",
    "    model.val_f1 = best_f1\n",
    "    model.optimal_threshold = best_threshold\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = model.feature_importance(importance_type='gain')\n",
    "    feature_importance = dict(zip(feature_cols, importance))\n",
    "    \n",
    "    # Print top 10 most important features\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TOP 10 MOST IMPORTANT FEATURES\")\n",
    "    print(\"=\"*50)\n",
    "    for i, (feature, importance) in enumerate(sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:10], 1):\n",
    "        print(f\"{i:2d}. {feature}: {importance:.2f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7059fef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T18:14:23.941040Z",
     "iopub.status.busy": "2025-07-20T18:14:23.940781Z",
     "iopub.status.idle": "2025-07-20T18:19:24.589578Z",
     "shell.execute_reply": "2025-07-20T18:19:24.588982Z",
     "shell.execute_reply.started": "2025-07-20T18:14:23.941018Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"STARTING MODEL TRAINING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = train_model(train_filtered, val_filtered)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL TRAINING COMPLETED\")\n",
    "print(\"=\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e824e7f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-20T13:27:58.829821Z",
     "iopub.status.idle": "2025-07-20T13:27:58.830023Z",
     "shell.execute_reply": "2025-07-20T13:27:58.829932Z",
     "shell.execute_reply.started": "2025-07-20T13:27:58.829924Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_top_features(train_df, model, k=50):\n",
    "    \"\"\"\n",
    "    Select top k features based on LightGBM feature importance.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_df: Training DataFrame\n",
    "    - model: Trained LightGBM model\n",
    "    - k: Number of top features to select\n",
    "    \n",
    "    Returns:\n",
    "    - List of top k feature names\n",
    "    \"\"\"\n",
    "    feature_cols = [col for col in train_df.columns if col not in ['y', 'id1', 'id3', 'id4', 'id5'] and not col.endswith('_dt')]\n",
    "    importance = model.feature_importance(importance_type='gain')\n",
    "    feature_importance = dict(zip(feature_cols, importance))\n",
    "    sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_features = [feature for feature, _ in sorted_features[:k]]\n",
    "    print(f\"Top {k} features: {top_features}\")\n",
    "    return top_features\n",
    "\n",
    "# Select top 10 features\n",
    "features = select_top_features(train_df, results, k=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47095c1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T18:19:34.835519Z",
     "iopub.status.busy": "2025-07-20T18:19:34.834947Z",
     "iopub.status.idle": "2025-07-20T18:19:34.845307Z",
     "shell.execute_reply": "2025-07-20T18:19:34.844606Z",
     "shell.execute_reply.started": "2025-07-20T18:19:34.835492Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "def calculate_map7(df: pd.DataFrame, model: lgb.Booster, use_threshold: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Calculate MAP@7 metric for the validation DataFrame using LightGBM probability predictions\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing validation data with 'id2' (customer ID) and 'y' columns\n",
    "    - model: Trained LightGBM model\n",
    "    - use_threshold: If True, use model's optimal threshold for ranking; if False, use raw probabilities\n",
    "    \n",
    "    Returns:\n",
    "    - Float representing the MAP@7 score\n",
    "    \"\"\"\n",
    "    print(\"Calculating MAP@7...\")\n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "    \n",
    "    # Check if id2 exists\n",
    "    if 'id2' not in df.columns:\n",
    "        raise KeyError(\"Column 'id2' not found in DataFrame. Ensure 'id2' is not dropped during preprocessing.\")\n",
    "    \n",
    "    # Automatically select feature columns (same as train_model)\n",
    "    exclude_cols = ['id1','id2', 'id3', 'id4', 'id5', 'id6', 'id7', 'id8', \n",
    "                    'id9', 'id10', 'id11', 'id12', 'id13', 'pred', 'y', 'y_pred']\n",
    "    exclude_cols.extend([col for col in df.columns if col.endswith('_dt')])\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    print(f\"Using {len(feature_cols)} features for prediction\")\n",
    "    \n",
    "    # Generate probability predictions\n",
    "    df = df.copy()\n",
    "    df['y_pred'] = model.predict(df[feature_cols], num_iteration=model.best_iteration)\n",
    "    \n",
    "    # If use_threshold is True, convert probabilities to binary scores using optimal threshold\n",
    "    if use_threshold and hasattr(model, 'optimal_threshold'):\n",
    "        print(f\"Using optimal threshold for ranking: {model.optimal_threshold:.4f}\")\n",
    "        df['y_pred'] = (df['y_pred'] > model.optimal_threshold).astype(int)\n",
    "    \n",
    "    # Verify prediction range\n",
    "    print(f\"Prediction range: min={df['y_pred'].min():.6f}, max={df['y_pred'].max():.6f}, mean={df['y_pred'].mean():.6f}\")\n",
    "    \n",
    "    customers = df['id2'].unique()\n",
    "    print(f\"Number of customers: {len(customers)}\")\n",
    "    \n",
    "    map_scores = []\n",
    "    customers_with_clicks = 0\n",
    "    total_clicks = 0\n",
    "    \n",
    "    for i, customer_id in enumerate(customers):\n",
    "        customer_df = df[df['id2'] == customer_id].copy()\n",
    "        \n",
    "        # Sort by prediction scores in descending order\n",
    "        customer_df = customer_df.sort_values('y_pred', ascending=False)\n",
    "        \n",
    "        # Get top 7 predictions\n",
    "        top7 = customer_df.head(7)\n",
    "        \n",
    "        # Calculate Average Precision (AP)\n",
    "        customer_clicks = top7['y'].sum()\n",
    "        total_clicks += customer_clicks\n",
    "        \n",
    "        if customer_clicks > 0:\n",
    "            customers_with_clicks += 1\n",
    "            precisions = []\n",
    "            hits = 0\n",
    "            for j, (_, row) in enumerate(top7.iterrows()):\n",
    "                if row['y'] == 1:\n",
    "                    hits += 1\n",
    "                    precisions.append(hits / (j + 1))\n",
    "            \n",
    "            if precisions:\n",
    "                map_scores.append(np.mean(precisions))\n",
    "        \n",
    "        # Progress logging\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"Processed {i+1}/{len(customers)} customers...\")\n",
    "    \n",
    "    final_map7 = np.mean(map_scores) if map_scores else 0.0\n",
    "    \n",
    "    print(\"MAP@7 calculation complete:\")\n",
    "    print(f\"  - Customers with clicks in top-7: {customers_with_clicks}/{len(customers)}\")\n",
    "    print(f\"  - Total clicks in top-7: {total_clicks}\")\n",
    "    print(f\"  - MAP@7 score: {final_map7:.6f}\")\n",
    "\n",
    "    return final_map7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ac938",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T18:19:38.127640Z",
     "iopub.status.busy": "2025-07-20T18:19:38.126933Z",
     "iopub.status.idle": "2025-07-20T18:24:13.241695Z",
     "shell.execute_reply": "2025-07-20T18:24:13.241043Z",
     "shell.execute_reply.started": "2025-07-20T18:19:38.127615Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# map7_score = calculate_map7(val_df, model,use_threshold=True)\n",
    "# print(f\"MAP@7 Score (Raw Probabilities): {map7_score:.6f}\")\n",
    "map7_score = calculate_map7(val_filtered,results, use_threshold=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211c32c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T18:24:19.030273Z",
     "iopub.status.busy": "2025-07-20T18:24:19.029636Z",
     "iopub.status.idle": "2025-07-20T18:24:19.033404Z",
     "shell.execute_reply": "2025-07-20T18:24:19.032779Z",
     "shell.execute_reply.started": "2025-07-20T18:24:19.030249Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# template = pd.read_csv('/kaggle/input/amex-data/template.csv')\n",
    "# template.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753ce3ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T18:24:19.407016Z",
     "iopub.status.busy": "2025-07-20T18:24:19.406799Z",
     "iopub.status.idle": "2025-07-20T18:24:38.195424Z",
     "shell.execute_reply": "2025-07-20T18:24:38.194611Z",
     "shell.execute_reply.started": "2025-07-20T18:24:19.406999Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "def prepare_test_data_for_lgb(test_df: pd.DataFrame, train_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare test data for LightGBM by converting object columns to numeric\n",
    "    Uses the same encoding as training data to ensure consistency\n",
    "    \"\"\"\n",
    "    test_df_copy = test_df.copy()\n",
    "    train_df_copy = train_df.copy()\n",
    "    \n",
    "    # Get feature columns (exclude target 'y' and ID columns)\n",
    "    feature_cols = [col for col in test_df_copy.columns if col not in ['y','id1', 'id2', 'id3', 'id4', 'id5']]\n",
    "    \n",
    "    # Find object columns\n",
    "    object_cols = []\n",
    "    for col in feature_cols:\n",
    "        if test_df_copy[col].dtype == 'object':\n",
    "            object_cols.append(col)\n",
    "    \n",
    "    print(f\"Found {len(object_cols)} object columns to convert in test data:\")\n",
    "    print(object_cols)\n",
    "    \n",
    "    # Convert object columns to numeric\n",
    "    for col in object_cols:\n",
    "        print(f\"Converting column: {col}\")\n",
    "        \n",
    "        # Combine train and test data for consistent encoding\n",
    "        combined_data = pd.concat([train_df_copy[col] if col in train_df_copy.columns else pd.Series(), \n",
    "                                   test_df_copy[col]], axis=0)\n",
    "        \n",
    "        # Try to convert to numeric first\n",
    "        try:\n",
    "            combined_numeric = pd.to_numeric(combined_data, errors='coerce')\n",
    "            if combined_numeric.notna().sum() > len(combined_data) * 0.8:\n",
    "                test_df_copy[col] = pd.to_numeric(test_df_copy[col], errors='coerce')\n",
    "                print(f\"  -> Converted to numeric\")\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # If not numeric, use label encoding\n",
    "        le = LabelEncoder()\n",
    "        combined_data_filled = combined_data.fillna('missing').astype(str)\n",
    "        le.fit(combined_data_filled)\n",
    "        \n",
    "        test_filled = test_df_copy[col].fillna('missing').astype(str)\n",
    "        test_filled_encoded = []\n",
    "        for val in test_filled:\n",
    "            if val in le.classes_:\n",
    "                test_filled_encoded.append(le.transform([val])[0])\n",
    "            else:\n",
    "                test_filled_encoded.append(-1)  # Use -1 for unseen labels\n",
    "        \n",
    "        test_df_copy[col] = test_filled_encoded\n",
    "        print(f\"  -> Label encoded ({len(le.classes_)} unique values)\")\n",
    "    \n",
    "    # Convert any remaining object columns to numeric\n",
    "    for col in feature_cols:\n",
    "        if test_df_copy[col].dtype == 'object':\n",
    "            test_df_copy[col] = pd.to_numeric(test_df_copy[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    print(\"\\nFinal test data types:\")\n",
    "    feature_dtypes = test_df_copy[feature_cols].dtypes\n",
    "    object_remaining = feature_dtypes[feature_dtypes == 'object']\n",
    "    if len(object_remaining) > 0:\n",
    "        print(f\"WARNING: Still have object columns: {object_remaining.index.tolist()}\")\n",
    "    else:\n",
    "        print(\"✓ All feature columns are numeric\")\n",
    "    \n",
    "    return test_df_copy\n",
    "\n",
    "# %% [code]\n",
    "print(\"=\" * 50)\n",
    "print(\"PREPARING TEST DATA\")\n",
    "print(\"=\" * 50)\n",
    "test_df_prepared = prepare_test_data_for_lgb(test_filtered, train_filtered)\n",
    "\n",
    "# %% [code]\n",
    "print(\"=\" * 50)\n",
    "print(\"DEBUGGING: ALIGNING FEATURES TO MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get model features directly from the trained model\n",
    "model_features = results.feature_name()\n",
    "print(f\"Model expects {len(model_features)} features.\")\n",
    "\n",
    "# Add missing features in test data\n",
    "for col in model_features:\n",
    "    if col not in test_df_prepared.columns:\n",
    "        print(f\"Adding missing feature '{col}' to test data with value 0\")\n",
    "        test_df_prepared[col] = 0\n",
    "\n",
    "# Drop extra features not used in model\n",
    "extra_features = [col for col in test_df_prepared.columns if col not in model_features + ['id1', 'id2', 'id3', 'id4', 'id5']]\n",
    "if extra_features:\n",
    "    print(f\"Dropping extra features from test data: {extra_features}\")\n",
    "    test_df_prepared = test_df_prepared.drop(columns=extra_features)\n",
    "\n",
    "# Reorder test columns to match model order\n",
    "test_df_prepared = test_df_prepared[['id1', 'id2', 'id3', 'id4', 'id5'] + model_features]\n",
    "\n",
    "print(\"Final test data shape:\", test_df_prepared.shape)\n",
    "\n",
    "# %% [code]\n",
    "from scipy.special import expit  # For sigmoid\n",
    "\n",
    "def generate_test_predictions(test_df: pd.DataFrame, model: lgb.Booster) -> pd.DataFrame:\n",
    "    print(\"=\" * 50)\n",
    "    print(\"GENERATING TEST PREDICTIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    feature_cols = model.feature_name()\n",
    "    \n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "    print(f\"Using {len(feature_cols)} features for prediction\")\n",
    "    \n",
    "    raw_preds = model.predict(test_df[feature_cols], num_iteration=model.best_iteration)\n",
    "    \n",
    "    # Apply sigmoid to convert raw scores to probabilities\n",
    "    sigmoid_preds = expit(raw_preds)\n",
    "    \n",
    "    # Clip for numerical stability\n",
    "    test_predictions = np.clip(sigmoid_preds, 0.0, 1.0)\n",
    "    \n",
    "    print(f\"Prediction range after sigmoid: min={test_predictions.min():.6f}, max={test_predictions.max():.6f}, mean={test_predictions.mean():.6f}\")\n",
    "    \n",
    "    submission_df = pd.DataFrame({\n",
    "        'id1': test_df['id1'],\n",
    "        'id2': test_df['id2'],\n",
    "        'id3': test_df['id3'],\n",
    "        'id5': test_df['id5'],\n",
    "        'pred': test_predictions\n",
    "    })\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "submission_df = generate_test_predictions(test_df_prepared, results)\n",
    "\n",
    "# %% [code]\n",
    "print(\"=\" * 50)\n",
    "print(\"VERIFYING SUBMISSION FORMAT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "template_df = pd.read_csv('/kaggle/input/amex-data/template.csv')\n",
    "\n",
    "if submission_df.shape[0] == template_df.shape[0]:\n",
    "    print(\"✓ Row count matches template\")\n",
    "else:\n",
    "    print(f\"WARNING: Row count mismatch! Template: {template_df.shape[0]}, Submission: {submission_df.shape[0]}\")\n",
    "\n",
    "if list(submission_df.columns) == list(template_df.columns):\n",
    "    print(\"✓ Column names and order match template\")\n",
    "else:\n",
    "    print(\"WARNING: Column mismatch!\")\n",
    "    print(f\"Template columns: {template_df.columns.tolist()}\")\n",
    "    print(f\"Submission columns: {submission_df.columns.tolist()}\")\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "print(submission_df.head(10))\n",
    "\n",
    "print(f\"\\nPrediction stats: Min={submission_df['pred'].min():.6f}, Max={submission_df['pred'].max():.6f}, Mean={submission_df['pred'].mean():.6f}, Std={submission_df['pred'].std():.6f}\")\n",
    "\n",
    "# %% [code]\n",
    "submission_path = '/kaggle/working/submission.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"\\n✓ Submission saved to: {submission_path}\")\n",
    "\n",
    "print(\"\\nFinal submission info:\")\n",
    "print(f\"Total rows: {len(submission_df):,}\")\n",
    "print(f\"Unique id2: {submission_df['id2'].nunique():,}\")\n",
    "print(f\"Unique id3: {submission_df['id3'].nunique():,}\")\n",
    "\n",
    "if submission_df.isnull().any().any():\n",
    "    print(\"\\nWARNING: Missing values detected in submission!\")\n",
    "    print(submission_df.isnull().sum())\n",
    "else:\n",
    "    print(\"\\n✓ No missing values in submission\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SUBMISSION PREPARATION COMPLETE!\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b26c2c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T14:04:15.039277Z",
     "iopub.status.busy": "2025-07-20T14:04:15.038686Z",
     "iopub.status.idle": "2025-07-20T14:04:15.375059Z",
     "shell.execute_reply": "2025-07-20T14:04:15.374480Z",
     "shell.execute_reply.started": "2025-07-20T14:04:15.039255Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e90d1a4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7838256,
     "sourceId": 12576238,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.539901,
   "end_time": "2025-07-26T15:21:31.019809",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-26T15:21:16.479908",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
